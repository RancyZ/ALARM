{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a4a90f-d972-482f-9ff2-9563dfc077ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load Excel file\n",
    "file_path = \"../../LLM_output_generation/embedded_only_video_anomalies.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Model names\n",
    "model_names = [\"claude-3-5-sonnet\", \"flash\", \"gpt-4o-mini\", \"gpt4o\", \"pro\"]\n",
    "\n",
    "# Parse stringified NumPy arrays into Python lists\n",
    "def parse_embedding(x):\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            return np.fromstring(x.strip(\"[]\"), sep=\" \").tolist()\n",
    "        except:\n",
    "            return np.nan\n",
    "    return x\n",
    "\n",
    "# Apply parsing to all embedding columns\n",
    "for model in model_names:\n",
    "    for prefix in [\"reas\"]:\n",
    "        col = f\"{model}-{prefix}_emb\"\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(parse_embedding)\n",
    "\n",
    "# Compute pairwise cosine similarities for each (model1, model2) pair\n",
    "def compute_similarities(df, prefix):\n",
    "    similarities = pd.DataFrame()\n",
    "    similarities[\"video_name\"] = df[\"video_name\"]\n",
    "    for model1, model2 in itertools.combinations(model_names, 2):\n",
    "        col1 = f\"{model1}-{prefix}_emb\"\n",
    "        col2 = f\"{model2}-{prefix}_emb\"\n",
    "        sim_col = f\"{model1}_vs_{model2}_{prefix}_sim\"\n",
    "        sim_values = []\n",
    "        for v1, v2 in zip(df[col1], df[col2]):\n",
    "            try:\n",
    "                sim = cosine_similarity([v1], [v2])[0][0]\n",
    "            except:\n",
    "                sim = np.nan\n",
    "            sim_values.append(sim)\n",
    "        similarities[sim_col] = sim_values\n",
    "    return similarities\n",
    "\n",
    "# Compute similarities\n",
    "# desc_sim = compute_similarities(df, \"desc\")\n",
    "reas_sim = compute_similarities(df, \"reas\")\n",
    "\n",
    "# Get anomaly columns\n",
    "anomaly_df = df[[\"video_name\"] + [f\"{model}-anomaly\" for model in model_names]]\n",
    "\n",
    "# Combine all results\n",
    "final_df = desc_sim.merge(reas_sim, on=\"video_name\").merge(anomaly_df, on=\"video_name\")\n",
    "\n",
    "# Save to Excel\n",
    "final_df.to_excel(\"video_anomaly_similarities_output_reas.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85a1bbb-2a8b-467a-ac33-484d2b61e58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "suffix = \"reas\"\n",
    "\n",
    "\n",
    "# Load files\n",
    "anno_df = pd.read_csv('Annotation_vad_1203.csv')\n",
    "desc_df = pd.read_excel(f'video_anomaly_similarities_output_{suffix}.xlsx')\n",
    "\n",
    "# Process video names\n",
    "anno_df['Title'] = anno_df['Title'].str.replace('.mp4', '', regex=False)\n",
    "desc_df['video_name'] = desc_df['video_name'].str.replace('.mp4', '', regex=False)\n",
    "\n",
    "# Merge\n",
    "merged_df = pd.merge(anno_df, desc_df, left_on='Title', right_on='video_name')\n",
    "\n",
    "# Split by type with same ratio\n",
    "train_list = []\n",
    "test_list = []\n",
    "\n",
    "for label in ['Normal', 'Abnormal', 'Vague Abnormal']:\n",
    "    subset = merged_df[merged_df['Label'] == label]\n",
    "    train, test = train_test_split(subset, test_size=0.2, random_state=42)\n",
    "    train_list.append(train)\n",
    "    test_list.append(test)\n",
    "    print(len(test))\n",
    "\n",
    "# Combine and drop columns\n",
    "train_df = pd.concat(train_list).reset_index(drop=True).drop(columns=['Category', 'Label', 'Title'])\n",
    "test_df = pd.concat(test_list).reset_index(drop=True).drop(columns=['Category', 'Label','Title'])\n",
    "\n",
    "# Save results\n",
    "train_df.to_excel(f'train_dataset_{suffix}.xlsx', index=False)\n",
    "test_df.to_excel(f'test_dataset_{suffix}.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e0129e-4f99-4fd2-a025-b302adf54a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your file\n",
    "df = pd.read_excel(\"../../LLM_output_generation/embedded_only_video_anomalies.xlsx\")\n",
    "\n",
    "# Identify columns\n",
    "anomaly_cols = [col for col in df.columns if 'anomaly' in col.lower()]\n",
    "desc_cols = [col for col in df.columns if 'desc' in col.lower()]\n",
    "reas_cols = [col for col in df.columns if 'reas' in col.lower()]\n",
    "\n",
    "# --- For anomaly = 1 output ---\n",
    "df_anomaly1 = df.copy()\n",
    "for i in range(5):\n",
    "    anomaly_col = anomaly_cols[i]\n",
    "    desc_col = desc_cols[i]\n",
    "    reas_col = reas_cols[i]\n",
    "    \n",
    "    mask = df[anomaly_col] == 1\n",
    "    df_anomaly1.loc[~mask, [desc_col, reas_col]] = None  # Leave desc and reas as missing for anomaly = 0\n",
    "\n",
    "df_anomaly1.to_excel(\"video_outputs_anomaly1.xlsx\", index=False)\n",
    "\n",
    "\n",
    "# --- For anomaly = 0 output ---\n",
    "df_anomaly0 = df.copy()\n",
    "for i in range(5):\n",
    "    anomaly_col = anomaly_cols[i]\n",
    "    desc_col = desc_cols[i]\n",
    "    reas_col = reas_cols[i]\n",
    "    \n",
    "    mask = df[anomaly_col] == 0\n",
    "    df_anomaly0.loc[~mask, [desc_col, reas_col]] = None  # Leave desc and reas as missing for anomaly = 1\n",
    "\n",
    "df_anomaly0.to_excel(\"video_outputs_anomaly0.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1ae2b7-c04e-46b7-9192-7310d300e834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import re\n",
    "\n",
    "# Load your Excel file\n",
    "file_path = \"video_outputs_anomaly1.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Function to safely parse embedding strings with missing commas\n",
    "def safe_parse_emb(emb_str):\n",
    "    if isinstance(emb_str, str):\n",
    "        emb_str_clean = re.sub(r'([0-9e\\.\\+\\-])\\s+([\\-0-9])', r'\\1,\\2', emb_str)\n",
    "        try:\n",
    "            return np.array(ast.literal_eval(emb_str_clean))\n",
    "        except:\n",
    "            return np.nan\n",
    "    return np.nan\n",
    "\n",
    "# Convert numpy array to string: space-separated, square brackets, no commas\n",
    "def emb_to_str(emb):\n",
    "    return '[' + ' '.join(f\"{x:.8f}\" for x in emb) + ']'\n",
    "\n",
    "# Process each video row\n",
    "imputed_rows = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    video_result = row.copy()\n",
    "    desc_embs = []\n",
    "    reas_embs = []\n",
    "    \n",
    "    for model in ['claude-3-5-sonnet', 'flash', 'gpt-4o-mini', 'gpt4o', 'pro']:\n",
    "        desc_emb = safe_parse_emb(row[f'{model}-desc_emb'])\n",
    "        reas_emb = safe_parse_emb(row[f'{model}-reas_emb'])\n",
    "        anomaly = row[f'{model}-anomaly']\n",
    "        \n",
    "        if anomaly == 1 and isinstance(desc_emb, np.ndarray):\n",
    "            desc_embs.append(desc_emb)\n",
    "        if anomaly == 1 and isinstance(reas_emb, np.ndarray):\n",
    "            reas_embs.append(reas_emb)\n",
    "    \n",
    "    desc_mean = np.mean(desc_embs, axis=0) if desc_embs else None\n",
    "    reas_mean = np.mean(reas_embs, axis=0) if reas_embs else None\n",
    "    \n",
    "    for model in ['claude-3-5-sonnet', 'flash', 'gpt-4o-mini', 'gpt4o', 'pro']:\n",
    "        anomaly = row[f'{model}-anomaly']\n",
    "        \n",
    "        if anomaly == 0:\n",
    "            if desc_mean is not None:\n",
    "                video_result[f'{model}-desc_emb'] = emb_to_str(desc_mean)\n",
    "            if reas_mean is not None:\n",
    "                video_result[f'{model}-reas_emb'] = emb_to_str(reas_mean)\n",
    "    \n",
    "    imputed_rows.append(video_result)\n",
    "\n",
    "# Final DataFrame\n",
    "imputed_df = pd.DataFrame(imputed_rows)\n",
    "\n",
    "# Drop anomaly columns\n",
    "for model in ['claude-3-5-sonnet', 'flash', 'gpt-4o-mini', 'gpt4o', 'pro']:\n",
    "    if f'{model}-anomaly' in imputed_df.columns:\n",
    "        imputed_df.drop(columns=[f'{model}-anomaly'], inplace=True)\n",
    "\n",
    "# Save the result\n",
    "output_path = \"video_outputs_anomaly1_imputed_clean.xlsx\"\n",
    "imputed_df.to_excel(output_path, index=False)\n",
    "print(f\"Saved cleaned, imputed file to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08555d8e-7418-41ca-8a2f-4e6cb162bb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import re\n",
    "\n",
    "# Load your Excel file\n",
    "file_path = \"video_outputs_anomaly0.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Function to safely parse embedding strings with missing commas\n",
    "def safe_parse_emb(emb_str):\n",
    "    if isinstance(emb_str, str):\n",
    "        emb_str_clean = re.sub(r'([0-9e\\.\\+\\-])\\s+([\\-0-9])', r'\\1,\\2', emb_str)\n",
    "        try:\n",
    "            return np.array(ast.literal_eval(emb_str_clean))\n",
    "        except:\n",
    "            return np.nan\n",
    "    return np.nan\n",
    "\n",
    "# Convert numpy array to string: space-separated, square brackets, no commas\n",
    "def emb_to_str(emb):\n",
    "    return '[' + ' '.join(f\"{x:.8f}\" for x in emb) + ']'\n",
    "\n",
    "# Process each video row\n",
    "imputed_rows = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    video_result = row.copy()\n",
    "    desc_embs = []\n",
    "    reas_embs = []\n",
    "    \n",
    "    # Collect desc and reas embeddings for models with anomaly = 0\n",
    "    for model in ['claude-3-5-sonnet', 'flash', 'gpt-4o-mini', 'gpt4o', 'pro']:\n",
    "        desc_emb = safe_parse_emb(row[f'{model}-desc_emb'])\n",
    "        reas_emb = safe_parse_emb(row[f'{model}-reas_emb'])\n",
    "        anomaly = row[f'{model}-anomaly']\n",
    "        \n",
    "        if anomaly == 0 and isinstance(desc_emb, np.ndarray):\n",
    "            desc_embs.append(desc_emb)\n",
    "        if anomaly == 0 and isinstance(reas_emb, np.ndarray):\n",
    "            reas_embs.append(reas_emb)\n",
    "    \n",
    "    # Compute mean embeddings if available\n",
    "    desc_mean = np.mean(desc_embs, axis=0) if desc_embs else None\n",
    "    reas_mean = np.mean(reas_embs, axis=0) if reas_embs else None\n",
    "    \n",
    "    # Impute desc and reas for models with anomaly = 1\n",
    "    for model in ['claude-3-5-sonnet', 'flash', 'gpt-4o-mini', 'gpt4o', 'pro']:\n",
    "        anomaly = row[f'{model}-anomaly']\n",
    "        \n",
    "        if anomaly == 1:\n",
    "            if desc_mean is not None:\n",
    "                video_result[f'{model}-desc_emb'] = emb_to_str(desc_mean)\n",
    "            if reas_mean is not None:\n",
    "                video_result[f'{model}-reas_emb'] = emb_to_str(reas_mean)\n",
    "    \n",
    "    imputed_rows.append(video_result)\n",
    "\n",
    "# Final DataFrame\n",
    "imputed_df = pd.DataFrame(imputed_rows)\n",
    "\n",
    "# Drop anomaly columns\n",
    "for model in ['claude-3-5-sonnet', 'flash', 'gpt-4o-mini', 'gpt4o', 'pro']:\n",
    "    if f'{model}-anomaly' in imputed_df.columns:\n",
    "        imputed_df.drop(columns=[f'{model}-anomaly'], inplace=True)\n",
    "\n",
    "# Save the result\n",
    "output_path = \"video_outputs_anomaly0_imputed_clean.xlsx\"\n",
    "imputed_df.to_excel(output_path, index=False)\n",
    "print(f\"Saved cleaned, imputed file to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3085a8-0910-4af3-8cd6-38ad6191dae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the file\n",
    "file_path = \"video_outputs_anomaly0_imputed_clean.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Drop all desc_emb columns\n",
    "reas_cols = [col for col in df.columns if col.endswith('-desc_emb')]\n",
    "df.drop(columns=reas_cols, inplace=True)\n",
    "\n",
    "# Save the cleaned file\n",
    "output_path = \"video_outputs_anomaly0_reas.xlsx\"\n",
    "df.to_excel(output_path, index=False)\n",
    "print(f\"Saved to {output_path}\")\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the file\n",
    "file_path = \"video_outputs_anomaly1_imputed_clean.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Drop all desc_emb columns\n",
    "reas_cols = [col for col in df.columns if col.endswith('-desc_emb')]\n",
    "df.drop(columns=reas_cols, inplace=True)\n",
    "\n",
    "# Save the cleaned file\n",
    "output_path = \"video_outputs_anomaly1_reas.xlsx\"\n",
    "df.to_excel(output_path, index=False)\n",
    "print(f\"Saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053514a8-553e-4d0e-85f5-d44061af2cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load Excel file\n",
    "file_path = \"video_outputs_anomaly0_reas.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Model names\n",
    "model_names = [\"claude-3-5-sonnet\", \"flash\", \"gpt-4o-mini\", \"gpt4o\", \"pro\"]\n",
    "\n",
    "# Parse stringified NumPy arrays into Python lists\n",
    "def parse_embedding(x):\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            return np.fromstring(x.strip(\"[]\"), sep=\" \").tolist()\n",
    "        except:\n",
    "            return np.nan\n",
    "    return x\n",
    "\n",
    "# Apply parsing to all embedding columns\n",
    "for model in model_names:\n",
    "    for prefix in [\"reas\"]:\n",
    "        col = f\"{model}-{prefix}_emb\"\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(parse_embedding)\n",
    "\n",
    "# Compute pairwise cosine similarities for each (model1, model2) pair\n",
    "def compute_similarities(df, prefix):\n",
    "    similarities = pd.DataFrame()\n",
    "    similarities[\"video_name\"] = df[\"video_name\"]\n",
    "    for model1, model2 in itertools.combinations(model_names, 2):\n",
    "        col1 = f\"{model1}-{prefix}_emb\"\n",
    "        col2 = f\"{model2}-{prefix}_emb\"\n",
    "        sim_col = f\"{model1}_vs_{model2}_{prefix}_sim\"\n",
    "        sim_values = []\n",
    "        for v1, v2 in zip(df[col1], df[col2]):\n",
    "            try:\n",
    "                sim = cosine_similarity([v1], [v2])[0][0]\n",
    "            except:\n",
    "                sim = np.nan\n",
    "            sim_values.append(sim)\n",
    "        similarities[sim_col] = sim_values\n",
    "    return similarities\n",
    "\n",
    "# Compute similarities\n",
    "# desc_sim = compute_similarities(df, \"desc\")\n",
    "reas_sim = compute_similarities(df, \"reas\")\n",
    "\n",
    "# Get anomaly columns\n",
    "# anomaly_df = df[[\"video_name\"] + [f\"{model}-anomaly\" for model in model_names]]\n",
    "\n",
    "# Combine all results\n",
    "final_df = desc_sim.merge(reas_sim, on=\"video_name\")\n",
    "\n",
    "# Save to Excel\n",
    "final_df.to_excel(\"video_anomaly_similarities_output_reasoningUS_anomaly0_reas.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89af5087-8f91-466a-8ddc-0db7b13e8c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load Excel file\n",
    "file_path = \"video_outputs_anomaly1_reas.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Model names\n",
    "model_names = [\"claude-3-5-sonnet\", \"flash\", \"gpt-4o-mini\", \"gpt4o\", \"pro\"]\n",
    "\n",
    "# Parse stringified NumPy arrays into Python lists\n",
    "def parse_embedding(x):\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            return np.fromstring(x.strip(\"[]\"), sep=\" \").tolist()\n",
    "        except:\n",
    "            return np.nan\n",
    "    return x\n",
    "\n",
    "# Apply parsing to all embedding columns\n",
    "for model in model_names:\n",
    "    for prefix in [\"reas\"]:\n",
    "        col = f\"{model}-{prefix}_emb\"\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(parse_embedding)\n",
    "\n",
    "# Compute pairwise cosine similarities for each (model1, model2) pair\n",
    "def compute_similarities(df, prefix):\n",
    "    similarities = pd.DataFrame()\n",
    "    similarities[\"video_name\"] = df[\"video_name\"]\n",
    "    for model1, model2 in itertools.combinations(model_names, 2):\n",
    "        col1 = f\"{model1}-{prefix}_emb\"\n",
    "        col2 = f\"{model2}-{prefix}_emb\"\n",
    "        sim_col = f\"{model1}_vs_{model2}_{prefix}_sim\"\n",
    "        sim_values = []\n",
    "        for v1, v2 in zip(df[col1], df[col2]):\n",
    "            try:\n",
    "                sim = cosine_similarity([v1], [v2])[0][0]\n",
    "            except:\n",
    "                sim = np.nan\n",
    "            sim_values.append(sim)\n",
    "        similarities[sim_col] = sim_values\n",
    "    return similarities\n",
    "\n",
    "# Compute similarities\n",
    "# desc_sim = compute_similarities(df, \"desc\")\n",
    "reas_sim = compute_similarities(df, \"reas\")\n",
    "\n",
    "# Get anomaly columns\n",
    "# anomaly_df = df[[\"video_name\"] + [f\"{model}-anomaly\" for model in model_names]]\n",
    "\n",
    "# Combine all results\n",
    "final_df = desc_sim.merge(reas_sim, on=\"video_name\")\n",
    "\n",
    "# Save to Excel\n",
    "final_df.to_excel(\"video_anomaly_similarities_output_reasoningUS_anomaly1_reas.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b214700-778e-4919-92a1-84950bed13b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load training video names, ensure no .mp4 suffix\n",
    "train_df = pd.read_excel('../desc/train_dataset_desc.xlsx')\n",
    "train_videos = train_df['video_name'].astype(str).str.replace('.mp4', '', regex=False).unique()\n",
    "\n",
    "# Folder containing the files\n",
    "folder = \"./\"\n",
    "\n",
    "# File list to process\n",
    "file_list = [\n",
    "\n",
    "    \"video_anomaly_similarities_output_reasoningUS_anomaly0_reas.xlsx\",\n",
    "\n",
    "    \"video_anomaly_similarities_output_reasoningUS_anomaly1_reas.xlsx\"\n",
    "]\n",
    "\n",
    "# Process each file\n",
    "for file_name in file_list:\n",
    "    file_path = os.path.join(folder, file_name)\n",
    "    df = pd.read_excel(file_path)\n",
    "    \n",
    "    # Remove .mp4 suffix from video_name\n",
    "    df['video_name'] = df['video_name'].astype(str).str.replace('.mp4', '', regex=False)\n",
    "\n",
    "    # Extract suffix after 'output_'\n",
    "    suffix = file_name.split(\"output_\")[-1]\n",
    "\n",
    "    # Split into training and test sets\n",
    "    train_data = df[df['video_name'].isin(train_videos)].reset_index(drop=True)\n",
    "    test_data = df[~df['video_name'].isin(train_videos)].reset_index(drop=True)\n",
    "\n",
    "    # Output files to same folder\n",
    "    train_out = os.path.join(folder, f\"train_dataset_{suffix}\")\n",
    "    test_out = os.path.join(folder, f\"test_dataset_{suffix}\")\n",
    "\n",
    "    train_data.to_excel(train_out, index=False)\n",
    "    test_data.to_excel(test_out, index=False)\n",
    "\n",
    "    print(f\"Saved: {train_out} and {test_out}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7b8a6c-ce55-4edf-bf96-ef7ffca1cfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "suffix = \"reas\"\n",
    "\n",
    "# Load files\n",
    "anno_df = pd.read_csv('../desc/Annotation_vad_1203.csv')\n",
    "\n",
    "desc_df = pd.read_excel('train_dataset_reasoningUS_anomaly0_reas.xlsx')\n",
    "\n",
    "# desc_df = pd.read_excel('train_dataset_reasoningUS_anomaly1_reas.xlsx')\n",
    "\n",
    "\n",
    "\n",
    "# Clean video names\n",
    "anno_df['Title'] = anno_df['Title'].str.replace('.mp4', '', regex=False)\n",
    "desc_df['video_name'] = desc_df['video_name'].str.replace('.mp4', '', regex=False)\n",
    "\n",
    "# Merge\n",
    "merged_df = pd.merge(anno_df, desc_df, left_on='Title', right_on='video_name')\n",
    "\n",
    "# Optional: Create a copy of Label for stratification\n",
    "merged_df['strat_label'] = merged_df['Label']\n",
    "\n",
    "# Initialize StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Generate and save each training fold\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(merged_df, merged_df['strat_label'])):\n",
    "    train_fold = merged_df.iloc[train_idx].reset_index(drop=True)\n",
    "    # Drop unnecessary columns\n",
    "    train_fold = train_fold.drop(columns=['Category', 'Label', 'Title', 'strat_label'])\n",
    "\n",
    "    # Save to Excel\n",
    "    train_fold.to_excel(f'train_dataset_fold{fold}_{suffix}_anomaly0.xlsx', index=False)\n",
    "\n",
    "print(\"5-fold training splits saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0b4778-8ab5-4776-8dbc-9e246422c1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "suffix = \"reas\"\n",
    "\n",
    "# Load files\n",
    "anno_df = pd.read_csv('../desc/Annotation_vad_1203.csv')\n",
    "\n",
    "# desc_df = pd.read_excel('train_dataset_reasoningUS_anomaly0_reas.xlsx')\n",
    "\n",
    "desc_df = pd.read_excel('train_dataset_reasoningUS_anomaly1_reas.xlsx')\n",
    "\n",
    "\n",
    "\n",
    "# Clean video names\n",
    "anno_df['Title'] = anno_df['Title'].str.replace('.mp4', '', regex=False)\n",
    "desc_df['video_name'] = desc_df['video_name'].str.replace('.mp4', '', regex=False)\n",
    "\n",
    "# Merge\n",
    "merged_df = pd.merge(anno_df, desc_df, left_on='Title', right_on='video_name')\n",
    "\n",
    "# Optional: Create a copy of Label for stratification\n",
    "merged_df['strat_label'] = merged_df['Label']\n",
    "\n",
    "# Initialize StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Generate and save each training fold\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(merged_df, merged_df['strat_label'])):\n",
    "    train_fold = merged_df.iloc[train_idx].reset_index(drop=True)\n",
    "    # Drop unnecessary columns\n",
    "    train_fold = train_fold.drop(columns=['Category', 'Label', 'Title', 'strat_label'])\n",
    "\n",
    "    # Save to Excel\n",
    "    train_fold.to_excel(f'train_dataset_fold{fold}_{suffix}_anomaly1.xlsx', index=False)\n",
    "\n",
    "print(\"5-fold training splits saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03305b78-8eb5-4479-ad43-a9c22b5cf3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class PMF:\n",
    "    def __init__(self, num_users, num_items, num_factors, learning_rate=0.01, reg_param=0.01):\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.num_factors = num_factors\n",
    "        self.learning_rate = learning_rate\n",
    "        self.reg_param = reg_param\n",
    "        # Initialize U and V here, so they can be reset for grid search\n",
    "        self.U = np.random.normal(scale=1./self.num_factors, size=(self.num_users, self.num_factors))\n",
    "        self.V = np.random.normal(scale=1./self.num_factors, size=(self.num_items, self.num_factors))\n",
    "\n",
    "    def train(self, train_ratings, val_ratings, num_epochs=100, patience=5):\n",
    "        # train_ratings and val_ratings are lists of [i, j, rating]\n",
    "        \n",
    "        best_val_rmse = float('inf')\n",
    "        epochs_without_improvement = 0\n",
    "        \n",
    "        # Training using SGD\n",
    "        for epoch in range(num_epochs):\n",
    "            # Shuffle training data for each epoch\n",
    "            np.random.shuffle(train_ratings)\n",
    "            \n",
    "            for i, j, r_ij in train_ratings:\n",
    "                i, j = int(i), int(j)\n",
    "                prediction = self.predict(i, j)\n",
    "                error = r_ij - prediction\n",
    "\n",
    "                # Update latent factors\n",
    "                u_i = self.U[i, :].copy() # Make a copy before update\n",
    "                v_j = self.V[j, :].copy() # Make a copy before update\n",
    "                \n",
    "                self.U[i, :] += self.learning_rate * (error * v_j - self.reg_param * u_i)\n",
    "                self.V[j, :] += self.learning_rate * (error * u_i - self.reg_param * v_j)\n",
    "\n",
    "            # --- Validation Phase ---\n",
    "            val_rmse = self.compute_rmse(val_ratings)\n",
    "            print(f'Epoch: {epoch+1}, Validation RMSE: {val_rmse:.4f}')\n",
    "\n",
    "            # --- Early Stopping Logic ---\n",
    "            if val_rmse < best_val_rmse:\n",
    "                best_val_rmse = val_rmse\n",
    "                epochs_without_improvement = 0\n",
    "                # Optionally save the best model weights\n",
    "                best_U, best_V = self.U.copy(), self.V.copy()\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "            \n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f\"Stopping early after {epoch+1} epochs.\")\n",
    "                self.U, self.V = best_U, best_V # Restore best weights\n",
    "                break\n",
    "        \n",
    "        return best_val_rmse\n",
    "\n",
    "    def predict(self, i, j):\n",
    "        return np.dot(self.U[i, :], self.V[j, :])\n",
    "\n",
    "    def compute_rmse(self, ratings_data):\n",
    "        # ratings_data is a list of [i, j, rating]\n",
    "        if len(ratings_data) == 0:\n",
    "            return 0.0\n",
    "            \n",
    "        error = 0\n",
    "        for i, j, r_ij in ratings_data:\n",
    "            i, j = int(i), int(j)\n",
    "            error += (r_ij - self.predict(i, j)) ** 2\n",
    "        \n",
    "        # Return Root Mean Squared Error\n",
    "        return np.sqrt(error / len(ratings_data))\n",
    "\n",
    "    def full_matrix(self):\n",
    "        return np.dot(self.U, self.V.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d980119c-7e71-4dc8-a4f9-e83a4d0ec405",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# --- Main pipeline ---\n",
    "data_dir = \"./\"  # change if needed\n",
    "os.makedirs(\"low list\", exist_ok=True)\n",
    "\n",
    "for fold in range(5):\n",
    "    suffix = f\"fold{fold}_reas\"\n",
    "    train_file = os.path.join(data_dir, f\"train_dataset_{suffix}.xlsx\")\n",
    "    test_file = os.path.join(data_dir, f\"test_dataset_reas.xlsx\")\n",
    "   \n",
    "    df = pd.read_excel(train_file, sheet_name=\"Sheet1\")\n",
    "    R = df.select_dtypes(include=[np.number]).fillna(0).values\n",
    "    xs, ys = R.nonzero()\n",
    "    ratings = np.c_[xs, ys, R[xs, ys]]\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(ratings)\n",
    "    split_index = int(0.8 * len(ratings))\n",
    "    train_ratings = ratings[:split_index]\n",
    "    val_ratings = ratings[split_index:]\n",
    "    num_users, num_items = R.shape\n",
    "\n",
    "    param_grid = {\n",
    "        'num_factors': [5, 10, 15],\n",
    "        'learning_rate': [0.005, 0.01, 0.05],\n",
    "        'reg_param': [0.01, 0.1, 0.5]\n",
    "    }\n",
    "\n",
    "    best_rmse = float('inf')\n",
    "    for f in param_grid['num_factors']:\n",
    "        for lr in param_grid['learning_rate']:\n",
    "            for reg in param_grid['reg_param']:\n",
    "                pmf = PMF(num_users, num_items, f, lr, reg)\n",
    "                rmse = pmf.train(train_ratings, val_ratings, num_epochs=100, patience=5)\n",
    "                if rmse < best_rmse:\n",
    "                    best_rmse = rmse\n",
    "                    best_model = pmf\n",
    "\n",
    "    V = best_model.V\n",
    "    V_df = pd.DataFrame(V)\n",
    "    V_df.to_excel(f\"best_V_matrix_{suffix}.xlsx\", index=False)\n",
    "\n",
    "    test_df = pd.read_excel(test_file)\n",
    "    test_numeric = test_df.select_dtypes(include=[np.number]).apply(lambda x: x.fillna(x.mean()), axis=0).values\n",
    "    video_names = test_df['video_name'].astype(str).values\n",
    "\n",
    "    mse_list = []\n",
    "    c_list = []\n",
    "    for s in test_numeric:\n",
    "        c, _, _, _ = np.linalg.lstsq(V, s, rcond=None)\n",
    "        mse = mean_squared_error(s, V @ c)\n",
    "        mse_list.append(mse)\n",
    "        c_list.append(c)\n",
    "\n",
    "    result_df = pd.DataFrame(c_list, columns=[f'c{i}' for i in range(V.shape[1])])\n",
    "    result_df['MSE'] = mse_list\n",
    "    result_df.insert(0, 'video_name', video_names)\n",
    "    result_df.to_excel(f'optimal_c_and_mse_lstsq_with_name_{suffix}.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572d8771-995b-4775-af80-73a404cdf469",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "for fold in range(5):\n",
    "    suffix = f\"fold{fold}_reas\"\n",
    "    \n",
    "    # Load files\n",
    "    V_df = pd.read_excel(f'best_V_matrix_{suffix}.xlsx')\n",
    "    test_df = pd.read_excel(f'test_dataset_reasoningUS_anomaly1_reas.xlsx')\n",
    "  \n",
    "    \n",
    "    # Prepare matrix V and numeric part of test dataset\n",
    "    V = V_df.values\n",
    "    test_numeric_df = test_df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Fill missing values with column mean\n",
    "    test_numeric_df = test_numeric_df.apply(lambda x: x.fillna(x.mean()), axis=0)\n",
    "    S_numeric = test_numeric_df.values\n",
    "    \n",
    "    # Extract video names (assumes column 'video_name' exists)\n",
    "    video_names = test_df['video_name'].values\n",
    "    \n",
    "    # Solve for c minimizing ||s - Vc||^2 using least squares\n",
    "    mse_list = []\n",
    "    c_list = []\n",
    "    \n",
    "    for s in S_numeric:\n",
    "        c, residuals, _, _ = np.linalg.lstsq(V, s, rcond=None)\n",
    "        s_pred = V @ c\n",
    "        mse = mean_squared_error(s, s_pred)\n",
    "        mse_list.append(mse)\n",
    "        c_list.append(c)\n",
    "    \n",
    "    # Output results with video names\n",
    "    result_df = pd.DataFrame(c_list, columns=[f'c{i}' for i in range(V.shape[1])])\n",
    "    result_df['MSE'] = mse_list\n",
    "    result_df.insert(0, 'video_name', video_names)\n",
    "    \n",
    "    result_df.to_excel(f'optimal_c_and_mse_lstsq_with_name_{suffix}_anomaly1.xlsx', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b231d7a7-55b1-4676-9040-9bdeb1ad298e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "for fold in range(5):\n",
    "    suffix = f\"fold{fold}_reas\"\n",
    "    \n",
    "    # Load files\n",
    "    V_df = pd.read_excel(f'best_V_matrix_{suffix}.xlsx')\n",
    "    test_df = pd.read_excel(f'test_dataset_reasoningUS_anomaly0_reas.xlsx')\n",
    "  \n",
    "    \n",
    "    # Prepare matrix V and numeric part of test dataset\n",
    "    V = V_df.values\n",
    "    test_numeric_df = test_df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Fill missing values with column mean\n",
    "    test_numeric_df = test_numeric_df.apply(lambda x: x.fillna(x.mean()), axis=0)\n",
    "    S_numeric = test_numeric_df.values\n",
    "    \n",
    "    # Extract video names (assumes column 'video_name' exists)\n",
    "    video_names = test_df['video_name'].values\n",
    "    \n",
    "    # Solve for c minimizing ||s - Vc||^2 using least squares\n",
    "    mse_list = []\n",
    "    c_list = []\n",
    "    \n",
    "    for s in S_numeric:\n",
    "        c, residuals, _, _ = np.linalg.lstsq(V, s, rcond=None)\n",
    "        s_pred = V @ c\n",
    "        mse = mean_squared_error(s, s_pred)\n",
    "        mse_list.append(mse)\n",
    "        c_list.append(c)\n",
    "    \n",
    "    # Output results with video names\n",
    "    result_df = pd.DataFrame(c_list, columns=[f'c{i}' for i in range(V.shape[1])])\n",
    "    result_df['MSE'] = mse_list\n",
    "    result_df.insert(0, 'video_name', video_names)\n",
    "    \n",
    "    result_df.to_excel(f'optimal_c_and_mse_lstsq_with_name_{suffix}_anomaly0.xlsx', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f64992-8267-4aa8-be05-82344de145f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "for fold in range(5):\n",
    "    suffix = f\"fold{fold}_reas\"\n",
    "    \n",
    "    # Load files\n",
    "    V_df = pd.read_excel(f'best_V_matrix_{suffix}.xlsx')\n",
    "    test_df = pd.read_excel(f'train_dataset_{suffix}.xlsx')\n",
    "  \n",
    "    \n",
    "    # Prepare matrix V and numeric part of test dataset\n",
    "    V = V_df.values\n",
    "    test_numeric_df = test_df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Fill missing values with column mean\n",
    "    test_numeric_df = test_numeric_df.apply(lambda x: x.fillna(x.mean()), axis=0)\n",
    "    S_numeric = test_numeric_df.values\n",
    "    \n",
    "    # Extract video names (assumes column 'video_name' exists)\n",
    "    video_names = test_df['video_name'].values\n",
    "    \n",
    "    # Solve for c minimizing ||s - Vc||^2 using least squares\n",
    "    mse_list = []\n",
    "    c_list = []\n",
    "    \n",
    "    for s in S_numeric:\n",
    "        c, residuals, _, _ = np.linalg.lstsq(V, s, rcond=None)\n",
    "        s_pred = V @ c\n",
    "        mse = mean_squared_error(s, s_pred)\n",
    "        mse_list.append(mse)\n",
    "        c_list.append(c)\n",
    "    \n",
    "    # Output results with video names\n",
    "    result_df = pd.DataFrame(c_list, columns=[f'c{i}' for i in range(V.shape[1])])\n",
    "    result_df['MSE'] = mse_list\n",
    "    result_df.insert(0, 'video_name', video_names)\n",
    "    \n",
    "    result_df.to_excel(f'optimal_c_and_mse_lstsq_with_name_{suffix}_train.xlsx', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbbf21e-1db6-4a44-b4bd-2368f2142995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "for fold in range(5):\n",
    "    suffix = f\"fold{fold}_reas\"\n",
    "    \n",
    "    # Load files\n",
    "    V_df = pd.read_excel(f'best_V_matrix_{suffix}.xlsx')\n",
    "    test_df = pd.read_excel(f'train_dataset_{suffix}_anomaly1.xlsx')\n",
    "  \n",
    "    \n",
    "    # Prepare matrix V and numeric part of test dataset\n",
    "    V = V_df.values\n",
    "    test_numeric_df = test_df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Fill missing values with column mean\n",
    "    test_numeric_df = test_numeric_df.apply(lambda x: x.fillna(x.mean()), axis=0)\n",
    "    S_numeric = test_numeric_df.values\n",
    "    \n",
    "    # Extract video names (assumes column 'video_name' exists)\n",
    "    video_names = test_df['video_name'].values\n",
    "    \n",
    "    # Solve for c minimizing ||s - Vc||^2 using least squares\n",
    "    mse_list = []\n",
    "    c_list = []\n",
    "    \n",
    "    for s in S_numeric:\n",
    "        c, residuals, _, _ = np.linalg.lstsq(V, s, rcond=None)\n",
    "        s_pred = V @ c\n",
    "        mse = mean_squared_error(s, s_pred)\n",
    "        mse_list.append(mse)\n",
    "        c_list.append(c)\n",
    "    \n",
    "    # Output results with video names\n",
    "    result_df = pd.DataFrame(c_list, columns=[f'c{i}' for i in range(V.shape[1])])\n",
    "    result_df['MSE'] = mse_list\n",
    "    result_df.insert(0, 'video_name', video_names)\n",
    "    \n",
    "    result_df.to_excel(f'optimal_c_and_mse_lstsq_with_name_{suffix}_train_anomaly1.xlsx', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00c83ec-aba9-46e6-87e0-b73e0ab7cfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the provided files\n",
    "test_df = pd.read_excel('test_dataset_reasoningUS_anomaly1_reas.xlsx')\n",
    "anomalies_df = pd.read_excel('../../LLM_output_generation/embedded_only_video_anomalies.xlsx')\n",
    "\n",
    "# Remove .mp4 suffix for consistency\n",
    "test_df['video_name'] = test_df['video_name'].str.replace('.mp4', '', regex=False)\n",
    "anomalies_df['video_name'] = anomalies_df['video_name'].str.replace('.mp4', '', regex=False)\n",
    "\n",
    "# Identify all anomaly columns for different models\n",
    "anomaly_cols = [col for col in anomalies_df.columns if col.endswith('-anomaly')]\n",
    "\n",
    "# Keep relevant columns and reshape to long format\n",
    "anomaly_data = anomalies_df[['video_name'] + anomaly_cols]\n",
    "melted = anomaly_data.melt(id_vars='video_name', value_vars=anomaly_cols, \n",
    "                           var_name='model', value_name='anomaly')\n",
    "\n",
    "# Filter to videos in the test set\n",
    "filtered_melted = melted[melted['video_name'].isin(test_df['video_name'])]\n",
    "\n",
    "# Group by video_name and calculate percentage of anomaly = 1 and 0\n",
    "summary = filtered_melted.groupby('video_name')['anomaly'].value_counts(normalize=True).unstack(fill_value=0)\n",
    "summary = summary.rename(columns={0: 'percentage_anomaly_0', 1: 'percentage_anomaly_1'}).reset_index()\n",
    "\n",
    "# Save result to Excel\n",
    "summary.to_excel('anomaly_percentage_summary.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b7e4c6-5e61-4078-8e14-d7bea6ab28a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "for fold in range(5):\n",
    "    suffix = f\"fold{fold}_reas\"\n",
    "\n",
    "\n",
    "    \n",
    "    # Load the provided files\n",
    "    test_df = pd.read_excel(f'train_dataset_{suffix}_anomaly1.xlsx')\n",
    "    anomalies_df = pd.read_excel('embedded_only_video_anomalies.xlsx')\n",
    "    \n",
    "    # Remove .mp4 suffix for consistency\n",
    "    test_df['video_name'] = test_df['video_name'].str.replace('.mp4', '', regex=False)\n",
    "    anomalies_df['video_name'] = anomalies_df['video_name'].str.replace('.mp4', '', regex=False)\n",
    "    \n",
    "    # Identify all anomaly columns for different models\n",
    "    anomaly_cols = [col for col in anomalies_df.columns if col.endswith('-anomaly')]\n",
    "    \n",
    "    # Keep relevant columns and reshape to long format\n",
    "    anomaly_data = anomalies_df[['video_name'] + anomaly_cols]\n",
    "    melted = anomaly_data.melt(id_vars='video_name', value_vars=anomaly_cols, \n",
    "                               var_name='model', value_name='anomaly')\n",
    "    \n",
    "    # Filter to videos in the test set\n",
    "    filtered_melted = melted[melted['video_name'].isin(test_df['video_name'])]\n",
    "    \n",
    "    # Group by video_name and calculate percentage of anomaly = 1 and 0\n",
    "    summary = filtered_melted.groupby('video_name')['anomaly'].value_counts(normalize=True).unstack(fill_value=0)\n",
    "    summary = summary.rename(columns={0: 'percentage_anomaly_0', 1: 'percentage_anomaly_1'}).reset_index()\n",
    "    \n",
    "    # Save result to Excel\n",
    "    summary.to_excel(f'anomaly_percentage_summary_{suffix}_train.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9dbf70-1db9-4081-a80f-3a810b25df02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "for fold in range(5):\n",
    "    suffix = f\"fold{fold}_reas\"\n",
    "\n",
    "\n",
    "    \n",
    "    # Load required files\n",
    "    summary_df = pd.read_excel('anomaly_percentage_summary.xlsx')\n",
    "    mse_0_df = pd.read_excel(f'optimal_c_and_mse_lstsq_with_name_{suffix}_anomaly0.xlsx')\n",
    "    mse_1_df = pd.read_excel(f'optimal_c_and_mse_lstsq_with_name_{suffix}_anomaly1.xlsx')\n",
    "    \n",
    "    # Remove .mp4 suffix for consistency\n",
    "    summary_df['video_name'] = summary_df['video_name'].str.replace('.mp4', '', regex=False)\n",
    "    mse_0_df['video_name'] = mse_0_df['video_name'].str.replace('.mp4', '', regex=False)\n",
    "    mse_1_df['video_name'] = mse_1_df['video_name'].str.replace('.mp4', '', regex=False)\n",
    "    \n",
    "    # Merge all data on video_name\n",
    "    merged = summary_df.merge(mse_0_df[['video_name', 'MSE']], on='video_name', how='left')\n",
    "    merged = merged.rename(columns={'MSE': 'MSE_difference_0'})\n",
    "    \n",
    "    merged = merged.merge(mse_1_df[['video_name', 'MSE']], on='video_name', how='left')\n",
    "    merged = merged.rename(columns={'MSE': 'MSE_difference_1'})\n",
    "    \n",
    "    # Compute weighted MSE\n",
    "    merged['weighted_MSE'] = (\n",
    "        merged['percentage_anomaly_0'] * merged['MSE_difference_0'] +\n",
    "        merged['percentage_anomaly_1'] * merged['MSE_difference_1']\n",
    "    )\n",
    "    \n",
    "    # Output final file\n",
    "    merged[['video_name', 'weighted_MSE']].to_excel(f'weighted_mse_summary_{suffix}.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cda9e1-e856-4143-81b1-bedf69979034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "for fold in range(5):\n",
    "    suffix = f\"fold{fold}_reas\"\n",
    "\n",
    "\n",
    "    \n",
    "    # Load required files\n",
    "    summary_df = pd.read_excel(f'anomaly_percentage_summary_{suffix}_train.xlsx')\n",
    "    mse_0_df = pd.read_excel(f'optimal_c_and_mse_lstsq_with_name_{suffix}_train_anomaly0.xlsx')\n",
    "    mse_1_df = pd.read_excel(f'optimal_c_and_mse_lstsq_with_name_{suffix}_train_anomaly1.xlsx')\n",
    "    \n",
    "    # Remove .mp4 suffix for consistency\n",
    "    summary_df['video_name'] = summary_df['video_name'].str.replace('.mp4', '', regex=False)\n",
    "    mse_0_df['video_name'] = mse_0_df['video_name'].str.replace('.mp4', '', regex=False)\n",
    "    mse_1_df['video_name'] = mse_1_df['video_name'].str.replace('.mp4', '', regex=False)\n",
    "    \n",
    "    # Merge all data on video_name\n",
    "    merged = summary_df.merge(mse_0_df[['video_name', 'MSE']], on='video_name', how='left')\n",
    "    merged = merged.rename(columns={'MSE': 'MSE_difference_0'})\n",
    "    \n",
    "    merged = merged.merge(mse_1_df[['video_name', 'MSE']], on='video_name', how='left')\n",
    "    merged = merged.rename(columns={'MSE': 'MSE_difference_1'})\n",
    "    \n",
    "    # Compute weighted MSE\n",
    "    merged['weighted_MSE'] = (\n",
    "        merged['percentage_anomaly_0'] * merged['MSE_difference_0'] +\n",
    "        merged['percentage_anomaly_1'] * merged['MSE_difference_1']\n",
    "    )\n",
    "    \n",
    "    # Output final file\n",
    "    merged[['video_name', 'weighted_MSE']].to_excel(f'weighted_mse_summary_{suffix}_train.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a5089a-e53e-473c-81bf-bf6406fbfb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for fold in range(5):\n",
    "    suffix = f\"fold{fold}_reas\"\n",
    "\n",
    "    \n",
    "    # Load the two Excel files\n",
    "    df_desc = pd.read_excel(f'weighted_mse_summary_{suffix}.xlsx')\n",
    "    df_desc_reas = pd.read_excel(f'optimal_c_and_mse_lstsq_with_name_{suffix}.xlsx')\n",
    "    \n",
    "    # Ensure video_name has no .mp4 suffix if present\n",
    "    df_desc['video_name'] = df_desc['video_name'].str.replace('.mp4', '', regex=False)\n",
    "    df_desc_reas['video_name'] = df_desc_reas['video_name'].str.replace('.mp4', '', regex=False)\n",
    "    \n",
    "    # Merge on video_name\n",
    "    merged_df = pd.merge(df_desc, df_desc_reas, on='video_name')\n",
    "    \n",
    "    # Calculate MSE difference\n",
    "    merged_df['MSE_difference'] = merged_df['weighted_MSE'] - merged_df['MSE']\n",
    "    \n",
    "    # Output result\n",
    "    merged_df[['video_name', 'MSE_difference']].to_excel(f'mse_difference_reasoningUS_{suffix}.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b254904-f9d3-42ee-9767-78dfc22ab0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for fold in range(5):\n",
    "    suffix = f\"fold{fold}_reas\"\n",
    "\n",
    "    \n",
    "    # Load the two Excel files\n",
    "    df_desc = pd.read_excel(f'weighted_mse_summary_{suffix}_train.xlsx')\n",
    "    df_desc_reas = pd.read_excel(f'optimal_c_and_mse_lstsq_with_name_{suffix}_train.xlsx')\n",
    "    \n",
    "    # Ensure video_name has no .mp4 suffix if present\n",
    "    df_desc['video_name'] = df_desc['video_name'].str.replace('.mp4', '', regex=False)\n",
    "    df_desc_reas['video_name'] = df_desc_reas['video_name'].str.replace('.mp4', '', regex=False)\n",
    "    \n",
    "    # Merge on video_name\n",
    "    merged_df = pd.merge(df_desc, df_desc_reas, on='video_name')\n",
    "    \n",
    "    # Calculate MSE difference\n",
    "    merged_df['MSE_difference'] = merged_df['weighted_MSE'] - merged_df['MSE']\n",
    "    \n",
    "    # Output result\n",
    "    merged_df[['video_name', 'MSE_difference']].to_excel(f'mse_difference_reasoningUS_{suffix}_train.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9052117c-6a5e-401a-bf7b-7e6e2f1a3ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "for fold in range(5):\n",
    "    suffix = f\"fold{fold}_reas\"\n",
    "\n",
    "\n",
    "    \n",
    "    df = pd.read_excel(f'mse_difference_reasoningUS_{suffix}.xlsx')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    video_names = df[\"video_name\"].astype(str)\n",
    "    uncertainty_scores = df[\"MSE_difference\"].values\n",
    "    \n",
    "    # Percent of videos to exclude as high-uncertainty\n",
    "    P_set = [5, 10, 15, 20, 25, 30, 35, 40]\n",
    "    \n",
    "    # Process and save low-uncertainty videos\n",
    "    for P in P_set:\n",
    "        tau = np.percentile(uncertainty_scores, 100 - P)\n",
    "        mask_low = uncertainty_scores <= tau\n",
    "        S_low_videos = video_names[mask_low]\n",
    "    \n",
    "        out_df = pd.DataFrame({\n",
    "            \"video_name\": S_low_videos.values,\n",
    "            \"uncertainty\": uncertainty_scores[mask_low]\n",
    "        })\n",
    "        \n",
    "        out_df.to_excel(f\"low list/S_low_videos_{P}_{suffix}.xlsx\", index=False)\n",
    "        print(f\"Threshold Ï„ = {tau:.4f}\")\n",
    "        print(f\"Kept {len(S_low_videos)}/{len(video_names)} videos in S_low.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1d8240-f23c-4052-a78c-abdd668bc003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "for fold in range(5):\n",
    "    suffix = f\"fold{fold}_reas\"\n",
    "\n",
    "\n",
    "    \n",
    "    df = pd.read_excel(f'mse_difference_reasoningUS_{suffix}_train.xlsx')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    video_names = df[\"video_name\"].astype(str)\n",
    "    uncertainty_scores = df[\"MSE_difference\"].values\n",
    "    \n",
    "    # Percent of videos to exclude as high-uncertainty\n",
    "    P_set = [5, 10, 15, 20, 25, 30, 35, 40]\n",
    "    \n",
    "    # Process and save low-uncertainty videos\n",
    "    for P in P_set:\n",
    "        tau = np.percentile(uncertainty_scores, 100 - P)\n",
    "        mask_low = uncertainty_scores <= tau\n",
    "        S_low_videos = video_names[mask_low]\n",
    "    \n",
    "        out_df = pd.DataFrame({\n",
    "            \"video_name\": S_low_videos.values,\n",
    "            \"uncertainty\": uncertainty_scores[mask_low]\n",
    "        })\n",
    "        \n",
    "        out_df.to_excel(f\"low list/S_low_videos_{P}_{suffix}_train.xlsx\", index=False)\n",
    "        print(f\"Threshold Ï„ = {tau:.4f}\")\n",
    "        print(f\"Kept {len(S_low_videos)}/{len(video_names)} videos in S_low.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3e9c62-5044-49a2-aa42-4ffecb8926b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Model names\n",
    "model_names = [\"claude-3-5-sonnet\", \"flash\", \"gpt-4o-mini\", \"gpt4o\", \"pro\"]\n",
    "\n",
    "# P values representing percent of videos excluded as high-uncertainty\n",
    "P_set = [5, 10, 15, 20, 25, 30, 35, 40]\n",
    "\n",
    "\n",
    "for fold in range(5):\n",
    "    suffix = f\"fold{fold}_reas\"\n",
    "\n",
    "\n",
    "    \n",
    "    # Store results for each P\n",
    "    all_results = []\n",
    "    \n",
    "    for P in P_set:\n",
    "        \n",
    "        try:\n",
    "            low = pd.read_excel(f'low list/S_low_videos_{P}_{suffix}.xlsx')\n",
    "        except Exception as e:\n",
    "            print(f\"[Error] Failed to load S_low_videos_{P}_trace.xlsx: {e}\")\n",
    "            continue\n",
    "        \n",
    "        low_list = low['video_name'].str.replace('.mp4', '', regex=False)\n",
    "        \n",
    "        # Store predictions per model\n",
    "        model_preds = {}\n",
    "        \n",
    "        for model in model_names:\n",
    "            try:\n",
    "                df = pd.read_csv(f'../../LLM_output_generation/vad_results_{model}.csv')\n",
    "                df['Video Name'] = df['Video Name'].str.replace('.mp4', '', regex=False)\n",
    "                df = df[df['Video Name'].isin(low_list)]\n",
    "                model_preds[model] = df[['Video Name', 'Predicted Label']].set_index('Video Name')\n",
    "            except Exception as e:\n",
    "                print(f\"[Error] Failed to process {model}: {e}\")\n",
    "        \n",
    "        # Merge all predictions\n",
    "        merged = pd.DataFrame(index=low_list)\n",
    "        for model in model_names:\n",
    "            merged[model] = model_preds.get(model, pd.DataFrame()).reindex(low_list)['Predicted Label']\n",
    "        \n",
    "        # Drop rows with missing predictions\n",
    "        merged = merged.dropna()\n",
    "        \n",
    "        if merged.empty:\n",
    "            print(f\"[Warning] No valid data for P={P}, skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Load ground truth from claude-3-5-sonnet\n",
    "        gt_df = pd.read_csv('../../LLM_output_generation/vad_results_claude-3-5-sonnet.csv')\n",
    "        gt_df['Video Name'] = gt_df['Video Name'].str.replace('.mp4', '', regex=False)\n",
    "        gt_df = gt_df[gt_df['Video Name'].isin(merged.index)]\n",
    "        ground_truth = gt_df.set_index('Video Name')['True Label']\n",
    "        \n",
    "        # Majority voting\n",
    "        majority_vote = merged.mode(axis=1)[0]\n",
    "        \n",
    "        # Align ground truth\n",
    "        y_true = ground_truth.loc[merged.index]\n",
    "        \n",
    "        # Compute metrics\n",
    "        acc = accuracy_score(y_true, majority_vote)\n",
    "        prec = precision_score(y_true, majority_vote, zero_division=0)\n",
    "        rec = recall_score(y_true, majority_vote, zero_division=0)\n",
    "        f1 = f1_score(y_true, majority_vote, zero_division=0)\n",
    "        \n",
    "        all_results.append({\n",
    "            'P': P,\n",
    "            'Num Videos': len(merged),\n",
    "            'Accuracy': acc,\n",
    "            'Precision': prec,\n",
    "            'Recall': rec,\n",
    "            'F1-Score': f1\n",
    "        })\n",
    "        \n",
    "        print(f\"[P={P}] Processed {len(merged)} videos | Accuracy: {acc:.4f}, F1-Score: {f1:.4f}\")\n",
    "    \n",
    "    # Save final results to one Excel file\n",
    "    result_df = pd.DataFrame(all_results)\n",
    "    result_df.to_excel(f'low list/VAD_Majority_Voting_Summary_{suffix}.xlsx', index=False)\n",
    "    print(\"[Saved] VAD_Majority_Voting_Summary.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736e0058-d69f-4e2e-96f0-dcf1263320ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Model names\n",
    "model_names = [\"claude-3-5-sonnet\", \"flash\", \"gpt-4o-mini\", \"gpt4o\", \"pro\"]\n",
    "\n",
    "# P values representing percent of videos excluded as high-uncertainty\n",
    "P_set = [5, 10, 15, 20, 25, 30, 35, 40]\n",
    "\n",
    "\n",
    "for fold in range(5):\n",
    "    suffix = f\"fold{fold}_reas\"\n",
    "\n",
    "\n",
    "    \n",
    "    # Store results for each P\n",
    "    all_results = []\n",
    "    \n",
    "    for P in P_set:\n",
    "        \n",
    "        try:\n",
    "            low = pd.read_excel(f'low list/S_low_videos_{P}_{suffix}_train.xlsx')\n",
    "        except Exception as e:\n",
    "            print(f\"[Error] Failed to load S_low_videos_{P}_trace.xlsx: {e}\")\n",
    "            continue\n",
    "        \n",
    "        low_list = low['video_name'].str.replace('.mp4', '', regex=False)\n",
    "        \n",
    "        # Store predictions per model\n",
    "        model_preds = {}\n",
    "        \n",
    "        for model in model_names:\n",
    "            try:\n",
    "                df = pd.read_csv(f'../../LLM_output_generation/vad_results_{model}.csv')\n",
    "                df['Video Name'] = df['Video Name'].str.replace('.mp4', '', regex=False)\n",
    "                df = df[df['Video Name'].isin(low_list)]\n",
    "                model_preds[model] = df[['Video Name', 'Predicted Label']].set_index('Video Name')\n",
    "            except Exception as e:\n",
    "                print(f\"[Error] Failed to process {model}: {e}\")\n",
    "        \n",
    "        # Merge all predictions\n",
    "        merged = pd.DataFrame(index=low_list)\n",
    "        for model in model_names:\n",
    "            merged[model] = model_preds.get(model, pd.DataFrame()).reindex(low_list)['Predicted Label']\n",
    "        \n",
    "        # Drop rows with missing predictions\n",
    "        merged = merged.dropna()\n",
    "        \n",
    "        if merged.empty:\n",
    "            print(f\"[Warning] No valid data for P={P}, skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Load ground truth from claude-3-5-sonnet\n",
    "        gt_df = pd.read_csv('../../LLM_output_generation/vad_results_claude-3-5-sonnet.csv')\n",
    "        gt_df['Video Name'] = gt_df['Video Name'].str.replace('.mp4', '', regex=False)\n",
    "        gt_df = gt_df[gt_df['Video Name'].isin(merged.index)]\n",
    "        ground_truth = gt_df.set_index('Video Name')['True Label']\n",
    "        \n",
    "        # Majority voting\n",
    "        majority_vote = merged.mode(axis=1)[0]\n",
    "        \n",
    "        # Align ground truth\n",
    "        y_true = ground_truth.loc[merged.index]\n",
    "        \n",
    "        # Compute metrics\n",
    "        acc = accuracy_score(y_true, majority_vote)\n",
    "        prec = precision_score(y_true, majority_vote, zero_division=0)\n",
    "        rec = recall_score(y_true, majority_vote, zero_division=0)\n",
    "        f1 = f1_score(y_true, majority_vote, zero_division=0)\n",
    "        \n",
    "        all_results.append({\n",
    "            'P': P,\n",
    "            'Num Videos': len(merged),\n",
    "            'Accuracy': acc,\n",
    "            'Precision': prec,\n",
    "            'Recall': rec,\n",
    "            'F1-Score': f1\n",
    "        })\n",
    "        \n",
    "        print(f\"[P={P}] Processed {len(merged)} videos | Accuracy: {acc:.4f}, F1-Score: {f1:.4f}\")\n",
    "    \n",
    "    # Save final results to one Excel file\n",
    "    result_df = pd.DataFrame(all_results)\n",
    "    result_df.to_excel(f'low list/VAD_Majority_Voting_Summary_{suffix}_train.xlsx', index=False)\n",
    "    print(\"[Saved] VAD_Majority_Voting_Summary.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa5dd40-f3aa-4505-95f9-95d3e09959e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "for fold in range(5):\n",
    "    suffix = f\"fold{fold}_reas\"\n",
    "\n",
    "\n",
    "    \n",
    "    # Load the summary files\n",
    "    majority_df = pd.read_excel(f'low list/VAD_Majority_Voting_Summary_{suffix}.xlsx')\n",
    "    \n",
    "    # Extract metrics\n",
    "    P_values = majority_df['P']\n",
    "    overall_accuracy = majority_df['Accuracy']\n",
    "    # recall = majority_df['Recall']\n",
    "    # vague_abnormal_accuracy = vague_df['Accuracy']\n",
    "    \n",
    "    # Combine into final DataFrame\n",
    "    out_df = pd.DataFrame({\n",
    "        'P': P_values,\n",
    "        'Overall Accuracy': overall_accuracy\n",
    "        # 'Recall': recall,\n",
    "        # 'Vague Abnormal Accuracy': vague_abnormal_accuracy\n",
    "    })\n",
    "    \n",
    "    # Remove final two rows\n",
    "    out_df = out_df.iloc[:, :]\n",
    "    \n",
    "    # Save to xlsx\n",
    "    out_df.to_excel(f'low list/results_P_{suffix}.xlsx', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
