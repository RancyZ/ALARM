{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a5da15-d987-4cd0-8bd2-c798e369964e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import cv2\n",
    "import base64\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "\n",
    "from openai import OpenAI\n",
    "from google.cloud import storage\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=\"API_KEY\")\n",
    "\n",
    "# Convert image to base64 string\n",
    "def convert_image_to_base64(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    _, buffer = cv2.imencode(\".jpg\", image)\n",
    "    return base64.b64encode(buffer).decode(\"utf-8\")\n",
    "\n",
    "# Analyze a single image using GPT-4 API\n",
    "def analyze_image(client, image_path):\n",
    "    base64Image = convert_image_to_base64(image_path)\n",
    "\n",
    "\n",
    "\n",
    "    prompt_message = (\n",
    " \"You are a medical expert specializing in wound. You will be shown an image from a patient. Your task is to provide a prediction of the wound type in the image from the following seven categories: “Stab Wound”,  “Laceration”, “Ingrown Nails”, “Cut”, “Burns”, “Bruises”, “Abrasions”.\\n\"\n",
    "    \"Please respond using the format below:\\n\"\n",
    "    \"{\\n\"\n",
    "    '  \"description\": \"Provide a concise description of the image, including details such as size, shape, color, depth, exudate, tissue type (e.g., necrotic, granulating), surrounding skin condition, and any other relevant observations. (max 200 words)\"\\n'\n",
    "' \"reasoning\": \"Detailed reasoning for why this wound corresponds to the predicted category based on features. (max 100 words)\",\\n'\n",
    "' “Type\": One of the following: “Stab Wound”,  “Laceration”, “Ingrown Nails”, “Cut”, “Burns”, “Bruises”, “Abrasions”\\n'\n",
    "    \"}\\n\"\n",
    ")\n",
    "    \n",
    "\n",
    "\n",
    "    prompt_messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                prompt_message,\n",
    "                {\"image\": base64Image, \"resize\": 768},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    params = {\n",
    "        # \"model\": \"gpt-4o\",\n",
    "        \"model\": \"gpt-4o-mini-2024-07-18\",\n",
    "        \n",
    "        \"messages\": prompt_messages,\n",
    "        \"max_tokens\": 4096,\n",
    "        \"temperature\": 0.0\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        result = client.chat.completions.create(**params)\n",
    "        text = result.choices[0].message.content\n",
    "        return text, None\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing image: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# List images in the local directory and subdirectories\n",
    "def list_local_images(directory):\n",
    "    image_files = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                image_path = os.path.join(root, file)\n",
    "                relative_path = os.path.relpath(image_path, directory)\n",
    "                image_files.append(relative_path.replace(os.sep, '/'))\n",
    "    return image_files\n",
    "\n",
    "# Process images in batches\n",
    "def batch_process_images(client, images, directory, batch_size, output_filename):\n",
    "    responses = []\n",
    "    processing_times = []\n",
    "\n",
    "    def process_image(image):\n",
    "        image_path = os.path.join(directory, image)\n",
    "        start_time = time.time()\n",
    "        response_text, processing_time = analyze_image(client, image_path)\n",
    "        end_time = time.time()\n",
    "\n",
    "        if processing_time is None:\n",
    "            processing_time = end_time - start_time\n",
    "\n",
    "        response = {image: response_text}\n",
    "        with open(output_filename, \"a\") as f:\n",
    "            f.write(json.dumps(response) + \"\\n\")\n",
    "\n",
    "        return image, response_text, processing_time\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        for i in range(0, len(images), batch_size):\n",
    "            print(\"batch:\", i)\n",
    "            futures = []\n",
    "            batch = images[i:i + batch_size]\n",
    "            for image in batch:\n",
    "                futures.append(executor.submit(process_image, image))\n",
    "\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                image_name, response_text, processing_time = future.result()\n",
    "                responses.append({image_name: response_text})\n",
    "                processing_times.append((image_name, processing_time))\n",
    "    return responses, processing_times\n",
    "\n",
    "def save_responses_to_file(responses, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(responses, f, indent=4)\n",
    "\n",
    "def main_part(directory, model_type, batch_size):\n",
    "    images = list_local_images(directory)\n",
    "    print(f\"Found {len(images)} images in {directory} and subdirectories.\")\n",
    "\n",
    "    output_dir = \"response\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    jsonl_filename = os.path.join(output_dir, f'responses_{model_type}_images_hyp_1.jsonl')\n",
    "    json_filename = os.path.join(output_dir, f'responses_{model_type}_images_hyp_1.json')\n",
    "\n",
    "    responses, processing_times = batch_process_images(client, images, directory, batch_size, output_filename=jsonl_filename)\n",
    "\n",
    "    for response in responses:\n",
    "        print(response)\n",
    "\n",
    "    save_responses_to_file(responses, filename=json_filename)\n",
    "    print(\"Responses saved to json\")\n",
    "\n",
    "    df_time = pd.DataFrame(processing_times, columns=['Image Name', 'Processing Time'])\n",
    "    df_time['Image Name'] = df_time['Image Name'].apply(lambda x: os.path.splitext(os.path.basename(x))[0])\n",
    "\n",
    "    return jsonl_filename, df_time\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    current_dir = os.getcwd()\n",
    "    directory = os.path.join(current_dir, \"dataset\")  # Folder with images\n",
    "    batch_size = 10\n",
    "    model_type = 'gpt4o-mini'\n",
    "    filename, df_time = main_part(directory, model_type, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666e37a9-f22b-495a-9e14-5b97ea204bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import cv2\n",
    "import base64\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "\n",
    "from openai import OpenAI\n",
    "from google.cloud import storage\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "#load_dotenv()\n",
    "\n",
    "def extract_video_anomaly_results(filename):\n",
    "    anomaly_results = {}\n",
    "\n",
    "    # Read the JSON file line by line\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                # Parse each line as a dictionary\n",
    "                video_response = json.loads(line.strip())\n",
    "\n",
    "                for video, response_text in video_response.items():\n",
    "                    try:\n",
    "                        if response_text is None:\n",
    "                            raise json.JSONDecodeError(\"Response text is None\", response_text, 0)\n",
    "\n",
    "                        # Clean up the response text by removing unnecessary markers\n",
    "                        cleaned_response_text = response_text.strip('```json').strip()\n",
    "                        cleaned_response_text = cleaned_response_text.replace('\\n\\n', ' ').strip()\n",
    "\n",
    "                        # Parse the cleaned response text as JSON\n",
    "                        response_json = json.loads(cleaned_response_text)\n",
    "\n",
    "                        # Extract the video_description, reasoning, and anomaly values\n",
    "                        video_description = response_json.get('description', 'NAN')\n",
    "                        reasoning = response_json.get('reasoning', 'NAN')\n",
    "                        anomaly = response_json.get('Type', 0)\n",
    "\n",
    "                        # Store the extracted values in the desired format\n",
    "                        anomaly_results[video] = {\n",
    "                            \"description\": video_description,\n",
    "                            \"reasoning\": reasoning,\n",
    "                            \"Type\": anomaly\n",
    "                        }\n",
    "\n",
    "                    except (json.JSONDecodeError, AttributeError) as e:\n",
    "                        # Handle the case where response text is not valid JSON or other error\n",
    "                        anomaly_results[video] = {\n",
    "                            \"description\": \"NAN\",\n",
    "                            \"reasoning\": \"NAN\",\n",
    "                            \"Type\": 0  # Default to 0 for no anomaly detected\n",
    "                        }\n",
    "\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON from line: {line.strip()} - {e}\")\n",
    "                continue\n",
    "\n",
    "    return anomaly_results\n",
    "def load_and_format_rules(json_file_path):\n",
    "    # Load the JSON file\n",
    "    with open(json_file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Extract the rules list\n",
    "    rules = data.get('rules', [])\n",
    "\n",
    "    # Combine the rules into a single string\n",
    "    formatted_rules = \"\\n\".join(rules)\n",
    "    \n",
    "    return formatted_rules\n",
    "    \n",
    "def justify_anomaly_detection(anomaly_result, formatted_rules, client):\n",
    "    # Extract video description, reasoning, and anomaly from anomaly_result\n",
    "    video_description = anomaly_result.get('description', 'NAN')\n",
    "    reasoning = anomaly_result.get('reasoning', 'NAN')\n",
    "    anomaly = anomaly_result.get('Type', 'NAN')\n",
    "\n",
    "\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a medical expert specializing in wound. You are provided with the results of a wound analysis, including description, reasoning, and a wound type. Additionally, you have a set of reference examples for wound prediction. Some examples may be easily confused, e.g., abrasions vs. laceration, cut vs. laceration, and laceration vs. bruises.\n",
    "Your task is to review the provided examples. If the content matches any of the examples, apply the example and update the wound prediction result. If no example applies, retain the original prediction.\n",
    "\n",
    "The wound prediction result is:\n",
    "{{\n",
    "  \"description\": \"{video_description}\",\n",
    "  \"reasoning\": \"{reasoning}\",\n",
    "  \"Type\": \"{anomaly}\"\n",
    "}}\n",
    "\n",
    "# Reference Examples:\n",
    "1. Example 1:\n",
    "{{\n",
    "  \"description\": \"The image shows a wound on the skin with irregular, jagged edges. The wound is red and appears to have some depth, with possible minor bleeding. The surrounding skin is slightly red and inflamed, indicating irritation or trauma. There is no significant exudate visible, and the tissue appears raw but not necrotic.\",\n",
    "  \"reasoning\": \"The irregular, jagged edges and the appearance of the wound suggest it is an abrasion. Abrasions are characterized by jagged skin and minor bleeding. The absence of a puncture or burn pattern supports this classification.\",\n",
    "  \"Type\": \"Abrasions\"\n",
    "}}\n",
    "\n",
    "2. Example 2:\n",
    "{{\n",
    "  \"description\": \"The image shows a linear wound with clean, sharp edges, approximately several centimeters in length. The wound is open and appears to be of moderate depth, with visible red tissue and some bleeding. The surrounding skin is slightly reddened but otherwise intact. There is no significant swelling or bruising around the wound. The tissue within the wound appears fresh, with no signs of necrosis or granulation.\",\n",
    "  \"reasoning\": \"The characteristics of the wound, including its linear shape, clean edges, and moderate depth, are indicative of a Cut. Cuts are typically caused by sharp objects and result in clean cuts with bleeding, as seen in the image.\",\n",
    "  \"Type\": \"Cut\"\n",
    "}}\n",
    "\n",
    "3. Example 3:\n",
    "{{\n",
    "  \"description\": \"The image shows a wound with a reddish, irregular shape, surrounded by a purplish discoloration. The wound appears shallow with no significant depth and has a moist appearance. The surrounding skin shows signs of red and purple hues, indicating possible trauma. There is no visible necrotic tissue, and the area does not appear to have significant exudate. The skin around the wound is slightly swollen, suggesting inflammation.\",\n",
    "  \"reasoning\": \"The presence of a reddish, irregular wound with surrounding purplish discoloration and swelling suggests trauma. Blood vessels are damaged under the skin.\",\n",
    "  \"Type\": \"Laceration\"\n",
    "}}\n",
    "\n",
    "Please think step-by-step and respond using the format below:\n",
    "{{\n",
    "  \"Reasoning\": \"If the wound matches a reference example, provide reasoning based on the specific example. If no example applies, state 'No applicable rule; retaining the original result.'\",\n",
    "  \"updated_prediction\": \"One of the following: Stab Wound, Laceration, Ingrown Nails, Cut, Burns, Bruises, Abrasions\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    #print(prompt)\n",
    "    \n",
    "    params = {\n",
    "        # \"model\": \"gpt-4o\",\n",
    "        \"model\": \"gpt-4o-mini-2024-07-18\",\n",
    "\n",
    "        \n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a medical expert specializing in wound\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"max_tokens\": 4096,\n",
    "        \"temperature\": 0.0\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Call the OpenAI API\n",
    "        response = client.chat.completions.create(**params)\n",
    "        justification = response.choices[0].message.content\n",
    "        return justification\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing video description: {e}\")\n",
    "        return str(e)\n",
    "\n",
    "# Process videos in batches\n",
    "def batch_process_videos(videos_dict, batch_size, output_filename, client, formatted_rules):\n",
    "    responses = []\n",
    "    processing_times = []\n",
    "\n",
    "    def process_batch(batch):\n",
    "        batch_responses = {}\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Process each video in the batch\n",
    "        for video_id, description in batch.items():\n",
    "            result = justify_anomaly_detection(description, formatted_rules, client)\n",
    "            batch_responses[video_id] = result\n",
    "        \n",
    "        end_time = time.time()\n",
    "        processing_time = end_time - start_time\n",
    "\n",
    "        # Append the responses to the file\n",
    "        with open(output_filename, \"a\") as f:\n",
    "            for video_id, result in batch_responses.items():\n",
    "                response_text = {video_id: result}\n",
    "                f.write(json.dumps(response_text) + \"\\n\")\n",
    "\n",
    "        return batch_responses, processing_time\n",
    "\n",
    "    # Process videos in batches\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        video_items = list(videos_dict.items())  # Convert dictionary to list of tuples (video_id, description)\n",
    "        futures = []\n",
    "\n",
    "        for i in range(0, len(video_items), batch_size):\n",
    "            print(\"Processing batch:\", i // batch_size + 1)\n",
    "            batch = dict(video_items[i:i + batch_size])\n",
    "            future = executor.submit(process_batch, batch)\n",
    "            futures.append(future)\n",
    "        \n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            result, processing_time = future.result()\n",
    "            responses.append(result)\n",
    "            processing_times.append(processing_time)\n",
    "\n",
    "    return responses, processing_times\n",
    "    \n",
    "# Save responses to a JSON file\n",
    "def save_responses_to_file(responses, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(responses, f, indent=4)\n",
    "\n",
    "def main_part(videos_dict, formatted_rules, model_type, batch_size, client):\n",
    "    print(f\"Processing {len(videos_dict)} video descriptions.\")\n",
    "\n",
    "    # Create the directory if it doesn't exist\n",
    "    output_dir = \"ref\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Create file names based on the model_type\n",
    "    jsonl_filename = os.path.join(output_dir, f'rule10_{model_type}_1.jsonl')\n",
    "    json_filename = os.path.join(output_dir, f'rule10_{model_type}_1.json')\n",
    "\n",
    "    # Process video descriptions in batches and save to a JSONL file\n",
    "    responses, processing_times = batch_process_videos(videos_dict, batch_size, jsonl_filename, client, formatted_rules)\n",
    "\n",
    "    # Flatten the batch responses and save to JSON\n",
    "    flat_responses = {k: v for batch in responses for k, v in batch.items()}\n",
    "    save_responses_to_file(flat_responses, filename=json_filename)\n",
    "    print(\"Responses saved to JSON\")\n",
    "\n",
    "    # Create DataFrame with processing times\n",
    "    df_time = pd.DataFrame(enumerate(processing_times), columns=['Batch', 'Processing Time'])\n",
    "\n",
    "    return jsonl_filename, df_time\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Set up your OpenAI API key\n",
    "    client = OpenAI(api_key=\"API_KEY\")\n",
    "    # Get the video description first:\n",
    "    model_type = 'gpt4o-mini'\n",
    "    video_descriptions = extract_video_anomaly_results(f'response/responses_{model_type}_images_hyp_1.jsonl')\n",
    "    # test if the \"nan\" value is the same with the json\n",
    "    df = pd.DataFrame({\n",
    "            'Image Name': list(video_descriptions.keys()),\n",
    "            'Predicted Label': list(video_descriptions.values())\n",
    "        })\n",
    "    df.to_csv(f'test_rawreason_{model_type}.csv', index=False)\n",
    "    # load anomaly rules:\n",
    "    formatted_rules = load_and_format_rules('response/rule.json')\n",
    "    batch_size = 10  # Adjust batch size as needed\n",
    "    jsonl_filename, df_time = main_part(video_descriptions, formatted_rules, model_type, batch_size, client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35906c40-b3e2-428b-a015-bec4ad9e9bb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
