{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d13bfc-3990-45ab-b465-16d7814b5c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "suffix = \"reas\"\n",
    "\n",
    "\n",
    "# Load files\n",
    "anno_df = pd.read_excel('../wound_groundtruth.xlsx')\n",
    "\n",
    "\n",
    "#To obtain video_anomaly_similarities_output_reas.xlsx, we only need to keep columns regarding reasoning similarities in video_anomaly_similarities_output.xlsx\n",
    "\n",
    "desc_df = pd.read_excel(f'video_anomaly_similarities_output_{suffix}.xlsx')\n",
    "\n",
    "# Process video names\n",
    "anno_df['Image Name'] = anno_df['Image Name'].str.replace('.jpg', '', regex=False)\n",
    "desc_df['video_name'] = desc_df['video_name'].str.replace('.jpg', '', regex=False)\n",
    "\n",
    "# Merge\n",
    "merged_df = pd.merge(anno_df, desc_df, left_on='Image Name', right_on='video_name')\n",
    "\n",
    "# Split by type with same ratio\n",
    "train_list = []\n",
    "test_list = []\n",
    "\n",
    "for label in ['Abrasions', 'Ingrown Nails', 'Stab Wound', 'Bruises', 'Cut', 'Laceration', 'Burns']:\n",
    "    subset = merged_df[merged_df['Label'] == label]\n",
    "    train, test = train_test_split(subset, test_size=0.2, random_state=42)\n",
    "    train_list.append(train)\n",
    "    test_list.append(test)\n",
    "    print(len(test))\n",
    "\n",
    "# Combine and drop columns\n",
    "train_df = pd.concat(train_list).reset_index(drop=True).drop(columns=['Label', 'Image Name'])\n",
    "test_df = pd.concat(test_list).reset_index(drop=True).drop(columns=['Label','Image Name'])\n",
    "\n",
    "train_df.to_excel(f'train_dataset_{suffix}.xlsx', index=False)\n",
    "test_df.to_excel(f'test_dataset_{suffix}.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b611308-1a90-486b-b367-21b5ba9461c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "suffix = \"reas\"\n",
    "\n",
    "# Load files\n",
    "anno_df = pd.read_excel('../wound_groundtruth.xlsx')\n",
    "desc_df = pd.read_excel('train_dataset_reas.xlsx')\n",
    "\n",
    "# Clean video names\n",
    "anno_df['Image Name'] = anno_df['Image Name'].str.replace('.jpg', '', regex=False)\n",
    "desc_df['video_name'] = desc_df['video_name'].str.replace('.jpg', '', regex=False)\n",
    "\n",
    "# Merge datasets\n",
    "merged_df = pd.merge(anno_df, desc_df, left_on='Image Name', right_on='video_name')\n",
    "\n",
    "# Optional: Create a copy of Label for stratification\n",
    "merged_df['strat_label'] = merged_df['Label']\n",
    "\n",
    "# Initialize StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Generate and save each training fold\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(merged_df, merged_df['strat_label'])):\n",
    "    train_fold = merged_df.iloc[train_idx].reset_index(drop=True)\n",
    "    # Drop unnecessary columns\n",
    "    train_fold = train_fold.drop(columns=['Label', 'Image Name', 'strat_label'])\n",
    "\n",
    "    # Save to Excel\n",
    "    train_fold.to_excel(f'train_dataset_fold{fold}_{suffix}.xlsx', index=False)\n",
    "\n",
    "print(\"✅ 5-fold training splits saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e4ea3c-08b3-465a-a37a-70b1f781cff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file\n",
    "\n",
    "#To obtain embedded_only_video_anomalies_reas.xlsx, we only need to keep columns regarding reasoning embedding in embedded_only_video_anomalies.xlsx\n",
    "\n",
    "\n",
    "df = pd.read_excel(\"embedded_only_video_anomalies_reas.xlsx\")\n",
    "\n",
    "# Identify anomaly and reasoning columns\n",
    "anomaly_cols = [col for col in df.columns if 'anomaly' in col.lower()]\n",
    "reas_cols = [col for col in df.columns if 'reas' in col.lower()]\n",
    "\n",
    "# List of anomaly categories\n",
    "categories = [\"Abrasions\", \"Ingrown Nails\", \"Stab Wound\", \"Bruises\", \"Cut\", \"Laceration\", \"Burns\"]\n",
    "\n",
    "os.makedirs(\"intermediate data\", exist_ok=True)\n",
    "\n",
    "\n",
    "# Process each category\n",
    "for category in categories:\n",
    "    df_category = df.copy()\n",
    "    for i in range(5):\n",
    "        anomaly_col = anomaly_cols[i]\n",
    "        reas_col = reas_cols[i]\n",
    "        \n",
    "        # Keep only the rows where this model predicted the current category\n",
    "        mask = df[anomaly_col] == category\n",
    "        df_category.loc[~mask, [reas_col]] = None  # Remove reasoning if not predicted as this category\n",
    "\n",
    "    # Save to Excel\n",
    "    \n",
    "    filename = f\"intermediate data/video_outputs_anomaly_{category.replace(' ', '_')}.xlsx\"\n",
    "    df_category.to_excel(filename, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f98343-3e72-45a5-a310-ed09478b2aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import re\n",
    "import os\n",
    "\n",
    "# List of anomaly categories\n",
    "categories = [\"Abrasions\", \"Ingrown Nails\", \"Stab Wound\", \"Bruises\", \"Cut\", \"Laceration\", \"Burns\"]\n",
    "\n",
    "# Function to safely parse embedding strings with missing commas\n",
    "def safe_parse_emb(emb_str):\n",
    "    if isinstance(emb_str, str):\n",
    "        emb_str_clean = re.sub(r'([0-9e\\.\\+\\-])\\s+([\\-0-9])', r'\\1,\\2', emb_str)\n",
    "        try:\n",
    "            return np.array(ast.literal_eval(emb_str_clean))\n",
    "        except:\n",
    "            return np.nan\n",
    "    return np.nan\n",
    "\n",
    "# Convert numpy array to string: space-separated, square brackets, no commas\n",
    "def emb_to_str(emb):\n",
    "    return '[' + ' '.join(f\"{x:.8f}\" for x in emb) + ']'\n",
    "\n",
    "# List of models\n",
    "models = [\"claude3.5sonnet\", \"claude3.5\", \"claude3.7\", \"gpt4o-mini\", \"gpt4o\"]\n",
    "\n",
    "# Process each category\n",
    "for category in categories:\n",
    "    print(f\"Processing category: {category}\")\n",
    "    \n",
    "    file_path = f\"intermediate data/video_outputs_anomaly_{category.replace(' ', '_')}.xlsx\"\n",
    "    df = pd.read_excel(file_path)\n",
    "    imputed_rows = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        video_result = row.copy()\n",
    "        reas_embs = []\n",
    "\n",
    "        # Collect reasoning embeddings for matching predictions\n",
    "        for model in models:\n",
    "            reas_emb = safe_parse_emb(row.get(f'{model}-reas_emb', ''))\n",
    "            anomaly = row.get(f'{model}-anomaly', '')\n",
    "            if anomaly == category and isinstance(reas_emb, np.ndarray):\n",
    "                reas_embs.append(reas_emb)\n",
    "\n",
    "        # Compute mean reasoning embedding\n",
    "        reas_mean = np.mean(reas_embs, axis=0) if reas_embs else None\n",
    "\n",
    "        # Impute for models that did not predict this category\n",
    "        for model in models:\n",
    "            anomaly = row.get(f'{model}-anomaly', '')\n",
    "            if anomaly != category and reas_mean is not None:\n",
    "                video_result[f'{model}-reas_emb'] = emb_to_str(reas_mean)\n",
    "\n",
    "        imputed_rows.append(video_result)\n",
    "\n",
    "    # Final DataFrame\n",
    "    imputed_df = pd.DataFrame(imputed_rows)\n",
    "\n",
    "    # Drop anomaly columns\n",
    "    for model in models:\n",
    "        col = f'{model}-anomaly'\n",
    "        if col in imputed_df.columns:\n",
    "            imputed_df.drop(columns=[col], inplace=True)\n",
    "\n",
    "    # Save result\n",
    "    output_path = f\"intermediate data/video_outputs_anomaly_{category.replace(' ', '_')}_imputed_clean.xlsx\"\n",
    "    imputed_df.to_excel(output_path, index=False)\n",
    "    print(f\"Saved cleaned, imputed file to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a5199f-9695-4f94-a969-24d00a1e3d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Anomaly categories\n",
    "categories = [\"Abrasions\", \"Ingrown Nails\", \"Stab Wound\", \"Bruises\", \"Cut\", \"Laceration\", \"Burns\"]\n",
    "\n",
    "# Model names\n",
    "model_names = [\"claude3.5sonnet\", \"claude3.5\", \"claude3.7\", \"gpt4o-mini\", \"gpt4o\"]\n",
    "\n",
    "# Parse stringified NumPy arrays into Python lists\n",
    "def parse_embedding(x):\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            return np.fromstring(x.strip(\"[]\"), sep=\" \").tolist()\n",
    "        except:\n",
    "            return np.nan\n",
    "    return x\n",
    "\n",
    "# Compute pairwise cosine similarities for a given DataFrame and prefix\n",
    "def compute_similarities(df, prefix):\n",
    "    similarities = pd.DataFrame()\n",
    "    similarities[\"video_name\"] = df[\"video_name\"]\n",
    "    for model1, model2 in itertools.combinations(model_names, 2):\n",
    "        col1 = f\"{model1}-{prefix}_emb\"\n",
    "        col2 = f\"{model2}-{prefix}_emb\"\n",
    "        sim_col = f\"{model1}_vs_{model2}_{prefix}_sim\"\n",
    "        sim_values = []\n",
    "        for v1, v2 in zip(df[col1], df[col2]):\n",
    "            try:\n",
    "                sim = cosine_similarity([v1], [v2])[0][0]\n",
    "            except:\n",
    "                sim = np.nan\n",
    "            sim_values.append(sim)\n",
    "        similarities[sim_col] = sim_values\n",
    "    return similarities\n",
    "\n",
    "# Process all categories\n",
    "for category in categories:\n",
    "    print(f\"Processing category: {category}\")\n",
    "    cat_file = f\"intermediate data/video_outputs_anomaly_{category.replace(' ', '_')}_imputed_clean.xlsx\"\n",
    "    df = pd.read_excel(cat_file)\n",
    "\n",
    "    # Apply parsing to all reasoning embedding columns\n",
    "    for model in model_names:\n",
    "        col = f\"{model}-reas_emb\"\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(parse_embedding)\n",
    "\n",
    "    # Compute reasoning similarity\n",
    "    reas_sim = compute_similarities(df, \"reas\")\n",
    "\n",
    "    # Save result\n",
    "    output_path = f\"intermediate data/video_anomaly_similarities_output_reasoningUS_anomaly_{category.replace(' ', '_')}_reas.xlsx\"\n",
    "    reas_sim.to_excel(output_path, index=False)\n",
    "    print(f\"Saved: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2e0612-d8c4-4bb5-9d93-d23efd332c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# List of categories\n",
    "categories = [\"Abrasions\", \"Ingrown Nails\", \"Stab Wound\", \"Bruises\", \"Cut\", \"Laceration\", \"Burns\"]\n",
    "\n",
    "# Load ground truth\n",
    "anno_df = pd.read_excel('../wound_groundtruth.xlsx')\n",
    "anno_df['Image Name'] = anno_df['Image Name'].str.replace('.jpg', '', regex=False)\n",
    "\n",
    "# Process each category\n",
    "for category in categories:\n",
    "    suffix = f\"reasoningUS_anomaly_{category.replace(' ', '_')}_reas\"\n",
    "\n",
    "    # Load similarity file\n",
    "    sim_file = f\"intermediate data/video_anomaly_similarities_output_{suffix}.xlsx\"\n",
    "    desc_df = pd.read_excel(sim_file)\n",
    "    desc_df['video_name'] = desc_df['video_name'].str.replace('.jpg', '', regex=False)\n",
    "\n",
    "    # Merge ground truth with similarity data\n",
    "    merged_df = pd.merge(anno_df, desc_df, left_on='Image Name', right_on='video_name')\n",
    "\n",
    "\n",
    "    \n",
    "    # Split by type with same ratio\n",
    "    train_list = []\n",
    "    test_list = []\n",
    "    \n",
    "    for label in ['Abrasions', 'Ingrown Nails', 'Stab Wound', 'Bruises', 'Cut', 'Laceration', 'Burns']:\n",
    "        subset = merged_df[merged_df['Label'] == label]\n",
    "        train, test = train_test_split(subset, test_size=0.2, random_state=42)\n",
    "        train_list.append(train)\n",
    "        test_list.append(test)\n",
    "    \n",
    "        \n",
    "    # Combine and drop columns\n",
    "    train_df = pd.concat(train_list).reset_index(drop=True).drop(columns=['Label', 'Image Name'])\n",
    "    test_df = pd.concat(test_list).reset_index(drop=True).drop(columns=['Label','Image Name'])\n",
    "    \n",
    "    # Save results\n",
    "    train_df.to_excel(f'intermediate data/train_dataset_{suffix}.xlsx', index=False)\n",
    "    test_df.to_excel(f'intermediate data/test_dataset_{suffix}.xlsx', index=False)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499595ea-76f6-4801-b873-f314d4f6c252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "suffix = \"reas\"\n",
    "\n",
    "# Load files\n",
    "anno_df = pd.read_excel('../wound_groundtruth.xlsx')\n",
    "\n",
    "for cat_name in ['Abrasions', 'Ingrown_Nails', 'Stab_Wound', 'Bruises', 'Cut', 'Laceration', 'Burns']:\n",
    "\n",
    "    \n",
    "    desc_df = pd.read_excel(f'intermediate data/train_dataset_reasoningUS_anomaly_{cat_name}_reas.xlsx')\n",
    "    \n",
    "    # Clean video names\n",
    "    anno_df['Image Name'] = anno_df['Image Name'].str.replace('.jpg', '', regex=False)\n",
    "    desc_df['video_name'] = desc_df['video_name'].str.replace('.jpg', '', regex=False)\n",
    "    \n",
    "    # Merge datasets\n",
    "    merged_df = pd.merge(anno_df, desc_df, left_on='Image Name', right_on='video_name')\n",
    "    \n",
    "    # Optional: Create a copy of Label for stratification\n",
    "    merged_df['strat_label'] = merged_df['Label']\n",
    "    \n",
    "    # Initialize StratifiedKFold\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Generate and save each training fold\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(merged_df, merged_df['strat_label'])):\n",
    "        train_fold = merged_df.iloc[train_idx].reset_index(drop=True)\n",
    "        # Drop unnecessary columns\n",
    "        train_fold = train_fold.drop(columns=['Label', 'Image Name', 'strat_label'])\n",
    "    \n",
    "        # Save to Excel\n",
    "        train_fold.to_excel(f'train_dataset_fold{fold}_{suffix}_{cat_name}.xlsx', index=False)\n",
    "    \n",
    "    print(\"✅ 5-fold training splits saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab7273b-2106-4652-9316-d4e6d2f29435",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class PMF:\n",
    "    def __init__(self, num_users, num_items, num_factors, learning_rate=0.01, reg_param=0.01):\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.num_factors = num_factors\n",
    "        self.learning_rate = learning_rate\n",
    "        self.reg_param = reg_param\n",
    "        # Initialize U and V here, so they can be reset for grid search\n",
    "        self.U = np.random.normal(scale=1./self.num_factors, size=(self.num_users, self.num_factors))\n",
    "        self.V = np.random.normal(scale=1./self.num_factors, size=(self.num_items, self.num_factors))\n",
    "\n",
    "    def train(self, train_ratings, val_ratings, num_epochs=100, patience=5):\n",
    "        # train_ratings and val_ratings are lists of [i, j, rating]\n",
    "        \n",
    "        best_val_rmse = float('inf')\n",
    "        epochs_without_improvement = 0\n",
    "        \n",
    "        # Training using SGD\n",
    "        for epoch in range(num_epochs):\n",
    "            # Shuffle training data for each epoch\n",
    "            np.random.shuffle(train_ratings)\n",
    "            \n",
    "            for i, j, r_ij in train_ratings:\n",
    "                i, j = int(i), int(j)\n",
    "                prediction = self.predict(i, j)\n",
    "                error = r_ij - prediction\n",
    "\n",
    "                # Update latent factors\n",
    "                u_i = self.U[i, :].copy() # Make a copy before update\n",
    "                v_j = self.V[j, :].copy() # Make a copy before update\n",
    "                \n",
    "                self.U[i, :] += self.learning_rate * (error * v_j - self.reg_param * u_i)\n",
    "                self.V[j, :] += self.learning_rate * (error * u_i - self.reg_param * v_j)\n",
    "\n",
    "            # --- Validation Phase ---\n",
    "            val_rmse = self.compute_rmse(val_ratings)\n",
    "            print(f'Epoch: {epoch+1}, Validation RMSE: {val_rmse:.4f}')\n",
    "\n",
    "            # --- Early Stopping Logic ---\n",
    "            if val_rmse < best_val_rmse:\n",
    "                best_val_rmse = val_rmse\n",
    "                epochs_without_improvement = 0\n",
    "                # Optionally save the best model weights\n",
    "                best_U, best_V = self.U.copy(), self.V.copy()\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "            \n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f\"Stopping early after {epoch+1} epochs.\")\n",
    "                self.U, self.V = best_U, best_V # Restore best weights\n",
    "                break\n",
    "        \n",
    "        return best_val_rmse\n",
    "\n",
    "    def predict(self, i, j):\n",
    "        return np.dot(self.U[i, :], self.V[j, :])\n",
    "\n",
    "    def compute_rmse(self, ratings_data):\n",
    "        # ratings_data is a list of [i, j, rating]\n",
    "        if len(ratings_data) == 0:\n",
    "            return 0.0\n",
    "            \n",
    "        error = 0\n",
    "        for i, j, r_ij in ratings_data:\n",
    "            i, j = int(i), int(j)\n",
    "            error += (r_ij - self.predict(i, j)) ** 2\n",
    "        \n",
    "        # Return Root Mean Squared Error\n",
    "        return np.sqrt(error / len(ratings_data))\n",
    "\n",
    "    def full_matrix(self):\n",
    "        return np.dot(self.U, self.V.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d89ac2-945e-4f53-8d08-80dacb2ed752",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# --- Main pipeline ---\n",
    "data_dir = \"./\"  # change if needed\n",
    "os.makedirs(\"low list\", exist_ok=True)\n",
    "\n",
    "for fold in range(5):\n",
    "    suffix = f\"fold{fold}_reas\"\n",
    "    train_file = os.path.join(data_dir, f\"train_dataset_{suffix}.xlsx\")\n",
    "    test_file = os.path.join(data_dir, f\"test_dataset_reas.xlsx\")\n",
    "   \n",
    "    df = pd.read_excel(train_file, sheet_name=\"Sheet1\")\n",
    "    R = df.select_dtypes(include=[np.number]).fillna(0).values\n",
    "    xs, ys = R.nonzero()\n",
    "    ratings = np.c_[xs, ys, R[xs, ys]]\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(ratings)\n",
    "    split_index = int(0.8 * len(ratings))\n",
    "    train_ratings = ratings[:split_index]\n",
    "    val_ratings = ratings[split_index:]\n",
    "    num_users, num_items = R.shape\n",
    "\n",
    "    param_grid = {\n",
    "        'num_factors': [5, 10, 15],\n",
    "        'learning_rate': [0.005, 0.01, 0.05],\n",
    "        'reg_param': [0.01, 0.1, 0.5]\n",
    "    }\n",
    "\n",
    "    best_rmse = float('inf')\n",
    "    for f in param_grid['num_factors']:\n",
    "        for lr in param_grid['learning_rate']:\n",
    "            for reg in param_grid['reg_param']:\n",
    "                pmf = PMF(num_users, num_items, f, lr, reg)\n",
    "                rmse = pmf.train(train_ratings, val_ratings, num_epochs=100, patience=5)\n",
    "                if rmse < best_rmse:\n",
    "                    best_rmse = rmse\n",
    "                    best_model = pmf\n",
    "\n",
    "    V = best_model.V\n",
    "    V_df = pd.DataFrame(V)\n",
    "    V_df.to_excel(f\"best_V_matrix_{suffix}.xlsx\", index=False)\n",
    "\n",
    "    test_df = pd.read_excel(test_file)\n",
    "    test_numeric = test_df.select_dtypes(include=[np.number]).apply(lambda x: x.fillna(x.mean()), axis=0).values\n",
    "    video_names = test_df['video_name'].astype(str).values\n",
    "\n",
    "    mse_list = []\n",
    "    c_list = []\n",
    "    for s in test_numeric:\n",
    "        c, _, _, _ = np.linalg.lstsq(V, s, rcond=None)\n",
    "        mse = mean_squared_error(s, V @ c)\n",
    "        mse_list.append(mse)\n",
    "        c_list.append(c)\n",
    "\n",
    "    result_df = pd.DataFrame(c_list, columns=[f'c{i}' for i in range(V.shape[1])])\n",
    "    result_df['MSE'] = mse_list\n",
    "    result_df.insert(0, 'video_name', video_names)\n",
    "    result_df.to_excel(f'optimal_c_and_mse_lstsq_with_name_{suffix}.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876da717-4259-437b-8d50-dd0a37f16cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "\n",
    "for cat_name in ['Abrasions', 'Ingrown_Nails', 'Stab_Wound', 'Bruises', 'Cut', 'Laceration', 'Burns']:\n",
    "\n",
    "    \n",
    "    \n",
    "    for fold in range(5):\n",
    "        suffix = f\"fold{fold}_reas\"\n",
    "        \n",
    "        # Load files\n",
    "        V_df = pd.read_excel(f'best_V_matrix_{suffix}.xlsx')\n",
    "        test_df = pd.read_excel(f'intermediate data/test_dataset_reasoningUS_anomaly_{cat_name}_reas.xlsx')\n",
    "      \n",
    "        \n",
    "        # Prepare matrix V and numeric part of test dataset\n",
    "        V = V_df.values\n",
    "        test_numeric_df = test_df.select_dtypes(include=[np.number])\n",
    "        \n",
    "        # Fill missing values with column mean\n",
    "        test_numeric_df = test_numeric_df.apply(lambda x: x.fillna(x.mean()), axis=0)\n",
    "        S_numeric = test_numeric_df.values\n",
    "        \n",
    "        # Extract video names (assumes column 'video_name' exists)\n",
    "        video_names = test_df['video_name'].values\n",
    "        \n",
    "        # Solve for c minimizing ||s - Vc||^2 using least squares\n",
    "        mse_list = []\n",
    "        c_list = []\n",
    "        \n",
    "        for s in S_numeric:\n",
    "            c, residuals, _, _ = np.linalg.lstsq(V, s, rcond=None)\n",
    "            s_pred = V @ c\n",
    "            mse = mean_squared_error(s, s_pred)\n",
    "            mse_list.append(mse)\n",
    "            c_list.append(c)\n",
    "        \n",
    "        # Output results with video names\n",
    "        result_df = pd.DataFrame(c_list, columns=[f'c{i}' for i in range(V.shape[1])])\n",
    "        result_df['MSE'] = mse_list\n",
    "        result_df.insert(0, 'video_name', video_names)\n",
    "        \n",
    "        result_df.to_excel(f'optimal_c_and_mse_lstsq_with_name_{suffix}_{cat_name}.xlsx', index=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e402cc-bf0a-4235-8608-e479e6f30e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "for fold in range(5):\n",
    "    suffix = f\"fold{fold}_reas\"\n",
    "    \n",
    "    # Load files\n",
    "    V_df = pd.read_excel(f'best_V_matrix_{suffix}.xlsx')\n",
    "    test_df = pd.read_excel(f'train_dataset_{suffix}.xlsx')\n",
    "  \n",
    "    \n",
    "    # Prepare matrix V and numeric part of test dataset\n",
    "    V = V_df.values\n",
    "    test_numeric_df = test_df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Fill missing values with column mean\n",
    "    test_numeric_df = test_numeric_df.apply(lambda x: x.fillna(x.mean()), axis=0)\n",
    "    S_numeric = test_numeric_df.values\n",
    "    \n",
    "    # Extract video names (assumes column 'video_name' exists)\n",
    "    video_names = test_df['video_name'].values\n",
    "    \n",
    "    # Solve for c minimizing ||s - Vc||^2 using least squares\n",
    "    mse_list = []\n",
    "    c_list = []\n",
    "    \n",
    "    for s in S_numeric:\n",
    "        c, residuals, _, _ = np.linalg.lstsq(V, s, rcond=None)\n",
    "        s_pred = V @ c\n",
    "        mse = mean_squared_error(s, s_pred)\n",
    "        mse_list.append(mse)\n",
    "        c_list.append(c)\n",
    "    \n",
    "    # Output results with video names\n",
    "    result_df = pd.DataFrame(c_list, columns=[f'c{i}' for i in range(V.shape[1])])\n",
    "    result_df['MSE'] = mse_list\n",
    "    result_df.insert(0, 'video_name', video_names)\n",
    "    \n",
    "    result_df.to_excel(f'optimal_c_and_mse_lstsq_with_name_{suffix}_train.xlsx', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff9dba0-fba8-4366-b127-5bc58a689c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "\n",
    "for cat_name in ['Abrasions', 'Ingrown_Nails', 'Stab_Wound', 'Bruises', 'Cut', 'Laceration', 'Burns']:\n",
    "\n",
    "    \n",
    "    \n",
    "    for fold in range(5):\n",
    "        suffix = f\"fold{fold}_reas\"\n",
    "        \n",
    "        # Load files\n",
    "        V_df = pd.read_excel(f'best_V_matrix_{suffix}.xlsx')\n",
    "        test_df = pd.read_excel(f'intermediate data/train_dataset_{suffix}_{cat_name}.xlsx')\n",
    "      \n",
    "        \n",
    "        # Prepare matrix V and numeric part of test dataset\n",
    "        V = V_df.values\n",
    "        test_numeric_df = test_df.select_dtypes(include=[np.number])\n",
    "        \n",
    "        # Fill missing values with column mean\n",
    "        test_numeric_df = test_numeric_df.apply(lambda x: x.fillna(x.mean()), axis=0)\n",
    "        S_numeric = test_numeric_df.values\n",
    "        \n",
    "        # Extract video names (assumes column 'video_name' exists)\n",
    "        video_names = test_df['video_name'].values\n",
    "        \n",
    "        # Solve for c minimizing ||s - Vc||^2 using least squares\n",
    "        mse_list = []\n",
    "        c_list = []\n",
    "        \n",
    "        for s in S_numeric:\n",
    "            c, residuals, _, _ = np.linalg.lstsq(V, s, rcond=None)\n",
    "            s_pred = V @ c\n",
    "            mse = mean_squared_error(s, s_pred)\n",
    "            mse_list.append(mse)\n",
    "            c_list.append(c)\n",
    "        \n",
    "        # Output results with video names\n",
    "        result_df = pd.DataFrame(c_list, columns=[f'c{i}' for i in range(V.shape[1])])\n",
    "        result_df['MSE'] = mse_list\n",
    "        result_df.insert(0, 'video_name', video_names)\n",
    "        \n",
    "        result_df.to_excel(f'optimal_c_and_mse_lstsq_with_name_{suffix}_train_{cat_name}.xlsx', index=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd4174f-8867-4aa8-ab0d-aff4756d0266",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the provided files\n",
    "test_df = pd.read_excel('intermediate data/test_dataset_reasoningUS_anomaly_Cut_reas.xlsx')\n",
    "anomalies_df = pd.read_excel('embedded_only_video_anomalies_reas.xlsx')\n",
    "\n",
    "# Remove .mp4 suffix for consistency\n",
    "test_df['video_name'] = test_df['video_name'].str.replace('.jpg', '', regex=False)\n",
    "anomalies_df['video_name'] = anomalies_df['video_name'].str.replace('.jpg', '', regex=False)\n",
    "\n",
    "# Identify all anomaly columns for different models\n",
    "anomaly_cols = [col for col in anomalies_df.columns if col.endswith('-anomaly')]\n",
    "\n",
    "# Keep relevant columns and reshape to long format\n",
    "anomaly_data = anomalies_df[['video_name'] + anomaly_cols]\n",
    "melted = anomaly_data.melt(id_vars='video_name', value_vars=anomaly_cols, \n",
    "                           var_name='model', value_name='anomaly')\n",
    "\n",
    "# Filter to videos in the test set\n",
    "filtered_melted = melted[melted['video_name'].isin(test_df['video_name'])]\n",
    "\n",
    "# Group by video_name and calculate percentage of anomaly = 1 and 0\n",
    "summary = filtered_melted.groupby('video_name')['anomaly'].value_counts(normalize=True).unstack(fill_value=0)\n",
    "# summary = summary.rename(columns={0: 'percentage_anomaly_0', 1: 'percentage_anomaly_1'}).reset_index()\n",
    "\n",
    "# Rename category columns to 'percentage_{category}'\n",
    "summary = summary.rename(columns={col: f'percentage_{col}' for col in summary.columns if col != 'video_name'}).reset_index()\n",
    "\n",
    "# Save result to Excel\n",
    "summary.to_excel('anomaly_percentage_summary.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb636b2-1a84-4c46-bf27-ca805ace6c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "for fold in range(5):\n",
    "    suffix = f\"fold{fold}_reas\"\n",
    "\n",
    "\n",
    "    \n",
    "    # Load the provided files\n",
    "    test_df = pd.read_excel(f'intermediate data/train_dataset_{suffix}_Cut.xlsx')\n",
    "    anomalies_df = pd.read_excel('embedded_only_video_anomalies_reas.xlsx')\n",
    "    \n",
    "    # Remove .mp4 suffix for consistency\n",
    "    test_df['video_name'] = test_df['video_name'].str.replace('.jpg', '', regex=False)\n",
    "    anomalies_df['video_name'] = anomalies_df['video_name'].str.replace('.jpg', '', regex=False)\n",
    "    \n",
    "    # Identify all anomaly columns for different models\n",
    "    anomaly_cols = [col for col in anomalies_df.columns if col.endswith('-anomaly')]\n",
    "    \n",
    "    # Keep relevant columns and reshape to long format\n",
    "    anomaly_data = anomalies_df[['video_name'] + anomaly_cols]\n",
    "    melted = anomaly_data.melt(id_vars='video_name', value_vars=anomaly_cols, \n",
    "                               var_name='model', value_name='anomaly')\n",
    "    \n",
    "    # Filter to videos in the test set\n",
    "    filtered_melted = melted[melted['video_name'].isin(test_df['video_name'])]\n",
    "    \n",
    "    # Group by video_name and calculate percentage of anomaly = 1 and 0\n",
    "    summary = filtered_melted.groupby('video_name')['anomaly'].value_counts(normalize=True).unstack(fill_value=0)\n",
    "    # summary = summary.rename(columns={0: 'percentage_anomaly_0', 1: 'percentage_anomaly_1'}).reset_index()\n",
    "    \n",
    "    # Rename category columns to 'percentage_{category}'\n",
    "    summary = summary.rename(columns={col: f'percentage_{col}' for col in summary.columns if col != 'video_name'}).reset_index()\n",
    "\n",
    "    # Save result to Excel\n",
    "    summary.to_excel(f'anomaly_percentage_summary_{suffix}_train.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53201e11-33d9-4ed3-8d56-107a6dc74825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for fold in range(5):\n",
    "    suffix = f\"fold{fold}_reas\"\n",
    "\n",
    "    \n",
    "    # List of 7 categories\n",
    "    categories = [\"Abrasions\", \"Ingrown Nails\", \"Stab Wound\", \"Bruises\", \"Cut\", \"Laceration\", \"Burns\"]\n",
    "    \n",
    "    # Load summary file and clean video names\n",
    "    summary_df = pd.read_excel('anomaly_percentage_summary.xlsx')\n",
    "    summary_df['video_name'] = summary_df['video_name'].str.replace('.jpg', '', regex=False)\n",
    "    \n",
    "    # Initialize merged DataFrame\n",
    "    merged = summary_df.copy()\n",
    "    \n",
    "    # Loop through categories and merge MSEs\n",
    "    for category in categories:\n",
    "        #cat_key = category.replace(' ', ' ')\n",
    "        cat_key = category.replace(' ', '_')\n",
    "        \n",
    "        mse_file = f'optimal_c_and_mse_lstsq_with_name_{suffix}_{cat_key}.xlsx'\n",
    "        \n",
    "        mse_df = pd.read_excel(mse_file)\n",
    "        mse_df['video_name'] = mse_df['video_name'].str.replace('.jpg', '', regex=False)\n",
    "        merged = merged.merge(mse_df[['video_name', 'MSE']], on='video_name', how='left')\n",
    "        \n",
    "        # Rename MSE column to be category-specific\n",
    "        merged = merged.rename(columns={'MSE': f'MSE_difference_{cat_key}'})\n",
    "    \n",
    "    # Compute weighted MSE\n",
    "    merged['weighted_MSE'] = 0\n",
    "    for category in categories:\n",
    "        cat_key = category.replace(' ', '_')\n",
    "\n",
    "        #cat_key = category\n",
    "        pct_col = f'percentage_{category}'\n",
    "        mse_col = f'MSE_difference_{cat_key}'\n",
    "        #mse_col = f'MSE_difference_{category}'\n",
    "        #if pct_col in merged.columns and mse_col in merged.columns:\n",
    "        merged['weighted_MSE'] += merged[pct_col] * merged[mse_col]\n",
    "    \n",
    "    # Save result\n",
    "    merged[['video_name', 'weighted_MSE']].to_excel(f'weighted_mse_summary_{suffix}.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b779b12-25a2-42a8-afce-9b5c356b6898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for fold in range(5):\n",
    "    suffix = f\"fold{fold}_reas\"\n",
    "\n",
    "    \n",
    "    # List of 7 categories\n",
    "    categories = [\"Abrasions\", \"Ingrown Nails\", \"Stab Wound\", \"Bruises\", \"Cut\", \"Laceration\", \"Burns\"]\n",
    "    \n",
    "    # Load summary file and clean video names\n",
    "    summary_df = pd.read_excel(f'anomaly_percentage_summary_{suffix}_train.xlsx')\n",
    "    summary_df['video_name'] = summary_df['video_name'].str.replace('.jpg', '', regex=False)\n",
    "    \n",
    "    # Initialize merged DataFrame\n",
    "    merged = summary_df.copy()\n",
    "    \n",
    "    # Loop through categories and merge MSEs\n",
    "    for category in categories:\n",
    "        #cat_key = category.replace(' ', ' ')\n",
    "        cat_key = category.replace(' ', '_')\n",
    "        \n",
    "        mse_file = f'optimal_c_and_mse_lstsq_with_name_{suffix}_train_{cat_key}.xlsx'\n",
    "        \n",
    "        mse_df = pd.read_excel(mse_file)\n",
    "        mse_df['video_name'] = mse_df['video_name'].str.replace('.jpg', '', regex=False)\n",
    "        merged = merged.merge(mse_df[['video_name', 'MSE']], on='video_name', how='left')\n",
    "        \n",
    "        # Rename MSE column to be category-specific\n",
    "        merged = merged.rename(columns={'MSE': f'MSE_difference_{cat_key}'})\n",
    "    \n",
    "    # Compute weighted MSE\n",
    "    merged['weighted_MSE'] = 0\n",
    "    for category in categories:\n",
    "        cat_key = category.replace(' ', '_')\n",
    "\n",
    "        #cat_key = category\n",
    "        pct_col = f'percentage_{category}'\n",
    "        mse_col = f'MSE_difference_{cat_key}'\n",
    "        #mse_col = f'MSE_difference_{category}'\n",
    "        #if pct_col in merged.columns and mse_col in merged.columns:\n",
    "        merged['weighted_MSE'] += merged[pct_col] * merged[mse_col]\n",
    "    \n",
    "    # Save result\n",
    "    merged[['video_name', 'weighted_MSE']].to_excel(f'weighted_mse_summary_{suffix}_train.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463c698e-d955-4013-9f24-23f2d5578c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for fold in range(5):\n",
    "    suffix = f\"fold{fold}_reas\"\n",
    "\n",
    "    \n",
    "    # Load the two Excel files\n",
    "    df_desc = pd.read_excel(f'weighted_mse_summary_{suffix}.xlsx')\n",
    "    df_desc_reas = pd.read_excel(f'optimal_c_and_mse_lstsq_with_name_{suffix}.xlsx')\n",
    "    \n",
    "    # Ensure video_name has no .mp4 suffix if present\n",
    "    df_desc['video_name'] = df_desc['video_name'].str.replace('.mp4', '', regex=False)\n",
    "    df_desc_reas['video_name'] = df_desc_reas['video_name'].str.replace('.mp4', '', regex=False)\n",
    "    \n",
    "    # Merge on video_name\n",
    "    merged_df = pd.merge(df_desc, df_desc_reas, on='video_name')\n",
    "    \n",
    "    # Calculate MSE difference\n",
    "    merged_df['MSE_difference'] = merged_df['weighted_MSE'] - merged_df['MSE']\n",
    "    \n",
    "    # Output result\n",
    "    merged_df[['video_name', 'MSE_difference']].to_excel(f'mse_difference_reasoningUS_{suffix}.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355f1f5d-e4ba-4762-b3b0-be2bb79d3b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for fold in range(5):\n",
    "    suffix = f\"fold{fold}_reas\"\n",
    "\n",
    "    \n",
    "    # Load the two Excel files\n",
    "    df_desc = pd.read_excel(f'weighted_mse_summary_{suffix}_train.xlsx')\n",
    "    df_desc_reas = pd.read_excel(f'optimal_c_and_mse_lstsq_with_name_{suffix}_train.xlsx')\n",
    "    \n",
    "    # Ensure video_name has no .mp4 suffix if present\n",
    "    df_desc['video_name'] = df_desc['video_name'].str.replace('.mp4', '', regex=False)\n",
    "    df_desc_reas['video_name'] = df_desc_reas['video_name'].str.replace('.mp4', '', regex=False)\n",
    "    \n",
    "    # Merge on video_name\n",
    "    merged_df = pd.merge(df_desc, df_desc_reas, on='video_name')\n",
    "    \n",
    "    # Calculate MSE difference\n",
    "    merged_df['MSE_difference'] = merged_df['weighted_MSE'] - merged_df['MSE']\n",
    "    \n",
    "    # Output result\n",
    "    merged_df[['video_name', 'MSE_difference']].to_excel(f'mse_difference_reasoningUS_{suffix}_train.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dab5e2-e798-47f3-bc55-2badcef0b71e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "for fold in range(5):\n",
    "    suffix = f\"fold{fold}_reas\"\n",
    "\n",
    "\n",
    "    \n",
    "    df = pd.read_excel(f'mse_difference_reasoningUS_{suffix}.xlsx')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    video_names = df[\"video_name\"].astype(str)\n",
    "    uncertainty_scores = df[\"MSE_difference\"].values\n",
    "    \n",
    "    # Percent of videos to exclude as high-uncertainty\n",
    "    P_set = [5, 10, 15, 20, 25, 30, 35, 40]\n",
    "    \n",
    "    # Process and save low-uncertainty videos\n",
    "    for P in P_set:\n",
    "        tau = np.percentile(uncertainty_scores, 100 - P)\n",
    "        mask_low = uncertainty_scores <= tau\n",
    "        S_low_videos = video_names[mask_low]\n",
    "    \n",
    "        out_df = pd.DataFrame({\n",
    "            \"video_name\": S_low_videos.values,\n",
    "            \"uncertainty\": uncertainty_scores[mask_low]\n",
    "        })\n",
    "        \n",
    "        out_df.to_excel(f\"low list/S_low_videos_{P}_{suffix}.xlsx\", index=False)\n",
    "        print(f\"Threshold τ = {tau:.4f}\")\n",
    "        print(f\"Kept {len(S_low_videos)}/{len(video_names)} videos in S_low.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4af663-33bd-4bbf-bd04-0fa3cb1826c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "for fold in range(5):\n",
    "    suffix = f\"fold{fold}_reas\"\n",
    "\n",
    "\n",
    "    \n",
    "    df = pd.read_excel(f'mse_difference_reasoningUS_{suffix}_train.xlsx')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    video_names = df[\"video_name\"].astype(str)\n",
    "    uncertainty_scores = df[\"MSE_difference\"].values\n",
    "    \n",
    "    # Percent of videos to exclude as high-uncertainty\n",
    "    P_set = [5, 10, 15, 20, 25, 30, 35, 40]\n",
    "    \n",
    "    # Process and save low-uncertainty videos\n",
    "    for P in P_set:\n",
    "        tau = np.percentile(uncertainty_scores, 100 - P)\n",
    "        mask_low = uncertainty_scores <= tau\n",
    "        S_low_videos = video_names[mask_low]\n",
    "    \n",
    "        out_df = pd.DataFrame({\n",
    "            \"video_name\": S_low_videos.values,\n",
    "            \"uncertainty\": uncertainty_scores[mask_low]\n",
    "        })\n",
    "        \n",
    "        out_df.to_excel(f\"low list/S_low_videos_{P}_{suffix}_train.xlsx\", index=False)\n",
    "        print(f\"Threshold τ = {tau:.4f}\")\n",
    "        print(f\"Kept {len(S_low_videos)}/{len(video_names)} videos in S_low.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4ff470-8411-4ff4-9b89-0abd690dcb6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Model names\n",
    "model_names = [\n",
    "    \"claude3.5sonnet\",\n",
    "    \"claude3.5\",\n",
    "    \"claude3.7\",\n",
    "    \"gpt4o-mini\",\n",
    "    \"gpt4o\"\n",
    "]\n",
    "# P values representing percent of videos excluded as high-uncertainty\n",
    "P_set = [5, 10, 15, 20, 25, 30, 35, 40]\n",
    "\n",
    "\n",
    "for fold in range(5):\n",
    "    suffix = f\"fold{fold}_reas\"\n",
    "\n",
    "\n",
    "    \n",
    "    # Store results for each P\n",
    "    all_results = []\n",
    "    \n",
    "    for P in P_set:\n",
    "        \n",
    "        try:\n",
    "            low = pd.read_excel(f'low list/S_low_videos_{P}_{suffix}.xlsx')\n",
    "        except Exception as e:\n",
    "            print(f\"[Error] Failed to load S_low_videos_{P}_trace.xlsx: {e}\")\n",
    "            continue\n",
    "        \n",
    "        low_list = low['video_name'].str.replace('.mp4', '', regex=False)\n",
    "        \n",
    "        # Store predictions per model\n",
    "        model_preds = {}\n",
    "        \n",
    "        for model in model_names:\n",
    "            try:\n",
    "                df = pd.read_csv(f'../model_predictions/vad_results_{model}.csv')\n",
    "                df['Video Name'] = df['Video Name'].str.replace('.jpg', '', regex=False)\n",
    "                df = df[df['Video Name'].isin(low_list)]\n",
    "                model_preds[model] = df[['Video Name', 'Predicted Label']].set_index('Video Name')\n",
    "            except Exception as e:\n",
    "                print(f\"[Error] Failed to process {model}: {e}\")\n",
    "        \n",
    "        # Merge all predictions\n",
    "        merged = pd.DataFrame(index=low_list)\n",
    "        for model in model_names:\n",
    "            merged[model] = model_preds.get(model, pd.DataFrame()).reindex(low_list)['Predicted Label']\n",
    "        \n",
    "        # Drop rows with missing predictions\n",
    "        merged = merged.dropna()\n",
    "        \n",
    "        if merged.empty:\n",
    "            print(f\"[Warning] No valid data for P={P}, skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Load ground truth from claude-3-5-sonnet\n",
    "        gt_df = pd.read_csv('../model_predictions/vad_results_claude3.5sonnet.csv')\n",
    "        gt_df['Video Name'] = gt_df['Video Name'].str.replace('.mp4', '', regex=False)\n",
    "        gt_df = gt_df[gt_df['Video Name'].isin(merged.index)]\n",
    "        ground_truth = gt_df.set_index('Video Name')['True Label']\n",
    "        \n",
    "        # Majority voting\n",
    "        majority_vote = merged.mode(axis=1)[0]\n",
    "        \n",
    "        # Align ground truth\n",
    "        y_true = ground_truth.loc[merged.index]\n",
    "        \n",
    "        # Compute metrics\n",
    "        acc = accuracy_score(y_true, majority_vote)\n",
    "        prec = precision_score(y_true, majority_vote,  average='macro',zero_division=0)\n",
    "        rec = recall_score(y_true, majority_vote, average='macro', zero_division=0)\n",
    "        f1 = f1_score(y_true, majority_vote, average='macro', zero_division=0)\n",
    "        \n",
    "        all_results.append({\n",
    "            'P': P,\n",
    "            'Num Videos': len(merged),\n",
    "            'Accuracy': acc,\n",
    "            'Precision': prec,\n",
    "            'Recall': rec,\n",
    "            'F1-Score': f1\n",
    "        })\n",
    "        \n",
    "        print(f\"[P={P}] Processed {len(merged)} videos | Accuracy: {acc:.4f}, F1-Score: {f1:.4f}\")\n",
    "    \n",
    "    # Save final results to one Excel file\n",
    "    result_df = pd.DataFrame(all_results)\n",
    "    result_df.to_excel(f'low list/VAD_Majority_Voting_Summary_{suffix}.xlsx', index=False)\n",
    "    print(\"[Saved] VAD_Majority_Voting_Summary.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7764668-4104-43f5-9a00-da0cc7bb398a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Model names\n",
    "model_names = [\n",
    "    \"claude3.5sonnet\",\n",
    "    \"claude3.5\",\n",
    "    \"claude3.7\",\n",
    "    \"gpt4o-mini\",\n",
    "    \"gpt4o\"\n",
    "]\n",
    "# P values representing percent of videos excluded as high-uncertainty\n",
    "P_set = [5, 10, 15, 20, 25, 30, 35, 40]\n",
    "\n",
    "\n",
    "for fold in range(5):\n",
    "    suffix = f\"fold{fold}_reas\"\n",
    "\n",
    "\n",
    "    \n",
    "    # Store results for each P\n",
    "    all_results = []\n",
    "    \n",
    "    for P in P_set:\n",
    "        \n",
    "        try:\n",
    "            low = pd.read_excel(f'low list/S_low_videos_{P}_{suffix}_train.xlsx')\n",
    "        except Exception as e:\n",
    "            print(f\"[Error] Failed to load S_low_videos_{P}_trace.xlsx: {e}\")\n",
    "            continue\n",
    "        \n",
    "        low_list = low['video_name'].str.replace('.mp4', '', regex=False)\n",
    "        \n",
    "        # Store predictions per model\n",
    "        model_preds = {}\n",
    "        \n",
    "        for model in model_names:\n",
    "            try:\n",
    "                df = pd.read_csv(f'../model_predictions/vad_results_{model}.csv')\n",
    "                df['Video Name'] = df['Video Name'].str.replace('.jpg', '', regex=False)\n",
    "                df = df[df['Video Name'].isin(low_list)]\n",
    "                model_preds[model] = df[['Video Name', 'Predicted Label']].set_index('Video Name')\n",
    "            except Exception as e:\n",
    "                print(f\"[Error] Failed to process {model}: {e}\")\n",
    "        \n",
    "        # Merge all predictions\n",
    "        merged = pd.DataFrame(index=low_list)\n",
    "        for model in model_names:\n",
    "            merged[model] = model_preds.get(model, pd.DataFrame()).reindex(low_list)['Predicted Label']\n",
    "        \n",
    "        # Drop rows with missing predictions\n",
    "        merged = merged.dropna()\n",
    "        \n",
    "        if merged.empty:\n",
    "            print(f\"[Warning] No valid data for P={P}, skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Load ground truth from claude-3-5-sonnet\n",
    "        gt_df = pd.read_csv('../model_predictions/vad_results_claude3.5sonnet.csv')\n",
    "        gt_df['Video Name'] = gt_df['Video Name'].str.replace('.mp4', '', regex=False)\n",
    "        gt_df = gt_df[gt_df['Video Name'].isin(merged.index)]\n",
    "        ground_truth = gt_df.set_index('Video Name')['True Label']\n",
    "        \n",
    "        # Majority voting\n",
    "        majority_vote = merged.mode(axis=1)[0]\n",
    "        \n",
    "        # Align ground truth\n",
    "        y_true = ground_truth.loc[merged.index]\n",
    "        \n",
    "        # Compute metrics\n",
    "        acc = accuracy_score(y_true, majority_vote)\n",
    "        prec = precision_score(y_true, majority_vote,  average='macro',zero_division=0)\n",
    "        rec = recall_score(y_true, majority_vote, average='macro', zero_division=0)\n",
    "        f1 = f1_score(y_true, majority_vote, average='macro', zero_division=0)\n",
    "        \n",
    "        all_results.append({\n",
    "            'P': P,\n",
    "            'Num Videos': len(merged),\n",
    "            'Accuracy': acc,\n",
    "            'Precision': prec,\n",
    "            'Recall': rec,\n",
    "            'F1-Score': f1\n",
    "        })\n",
    "        \n",
    "        print(f\"[P={P}] Processed {len(merged)} videos | Accuracy: {acc:.4f}, F1-Score: {f1:.4f}\")\n",
    "    \n",
    "    # Save final results to one Excel file\n",
    "    result_df = pd.DataFrame(all_results)\n",
    "    result_df.to_excel(f'low list/VAD_Majority_Voting_Summary_{suffix}_train.xlsx', index=False)\n",
    "    print(\"[Saved] VAD_Majority_Voting_Summary.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b5ba39-0c31-498c-8b7a-b7b7d53d3b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "for fold in range(5):\n",
    "    suffix = f\"fold{fold}_reas\"\n",
    "\n",
    "\n",
    "    \n",
    "    # Load the summary files\n",
    "    majority_df = pd.read_excel(f'low list/VAD_Majority_Voting_Summary_{suffix}.xlsx')\n",
    "    \n",
    "    # Extract metrics\n",
    "    P_values = majority_df['P']\n",
    "    overall_accuracy = majority_df['Accuracy']\n",
    "    # recall = majority_df['Recall']\n",
    "    # vague_abnormal_accuracy = vague_df['Accuracy']\n",
    "    \n",
    "    # Combine into final DataFrame\n",
    "    out_df = pd.DataFrame({\n",
    "        'P': P_values,\n",
    "        'Overall Accuracy': overall_accuracy\n",
    "        # 'Recall': recall,\n",
    "        # 'Vague Abnormal Accuracy': vague_abnormal_accuracy\n",
    "    })\n",
    "    \n",
    "    # Remove final two rows\n",
    "    out_df = out_df.iloc[:, :]\n",
    "    \n",
    "    # Save to xlsx\n",
    "    out_df.to_excel(f'low list/results_P_{suffix}.xlsx', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
