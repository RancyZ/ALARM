{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257459b4-7aca-4615-b8d5-a4b1fae78bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load Excel file\n",
    "file_path = \"../../LLM_output_generation/extracted_video_anomalies_all_models.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Initialize model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Define model names\n",
    "model_names = [\n",
    "    \"claude3.5sonnet\",\n",
    "    \"claude3.5\",\n",
    "    \"claude3.7\",\n",
    "    \"gpt4o-mini\",\n",
    "    \"gpt4o\"\n",
    "]\n",
    "\n",
    "# Prepare final DataFrame\n",
    "final_data = pd.DataFrame()\n",
    "final_data[\"video_name\"] = df[\"video_name\"]\n",
    "\n",
    "# Embed and extract selected fields\n",
    "for model_name in model_names:\n",
    "    desc_col = f\"{model_name}-description\"\n",
    "    reas_col = f\"{model_name}-reasoning\"\n",
    "    anomaly_col = f\"{model_name}-anomaly\"\n",
    "\n",
    "    if desc_col in df.columns:\n",
    "        desc_texts = df[desc_col].tolist()\n",
    "        desc_embeds = []\n",
    "        for text in desc_texts:\n",
    "            if pd.isna(text):\n",
    "                desc_embeds.append(np.nan)\n",
    "            else:\n",
    "                desc_embeds.append(model.encode(text))\n",
    "        final_data[f\"{model_name}-desc_emb\"] = desc_embeds\n",
    "\n",
    "    if reas_col in df.columns:\n",
    "        reas_texts = df[reas_col].tolist()\n",
    "        reas_embeds = []\n",
    "        for text in reas_texts:\n",
    "            if pd.isna(text):\n",
    "                reas_embeds.append(np.nan)\n",
    "            else:\n",
    "                reas_embeds.append(model.encode(text))\n",
    "        final_data[f\"{model_name}-reas_emb\"] = reas_embeds\n",
    "\n",
    "    if anomaly_col in df.columns:\n",
    "        final_data[f\"{model_name}-anomaly\"] = df[anomaly_col]\n",
    "\n",
    "# Save to output file\n",
    "output_path = \"embedded_only_video_anomalies.xlsx\"\n",
    "final_data.to_excel(output_path, index=False)\n",
    "\n",
    "print(f\"✅ Done! File saved as: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84a7c58-f1ca-4c3c-b0a9-71f049e7a5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load Excel file\n",
    "file_path = \"embedded_only_video_anomalies.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Model names\n",
    "model_names = [\n",
    "    \"claude3.5sonnet\",\n",
    "    \"claude3.5\",\n",
    "    \"claude3.7\",\n",
    "    \"gpt4o-mini\",\n",
    "    \"gpt4o\"\n",
    "]\n",
    "\n",
    "# Parse stringified NumPy arrays into Python lists\n",
    "def parse_embedding(x):\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            return np.fromstring(x.strip(\"[]\"), sep=\" \").tolist()\n",
    "        except:\n",
    "            return np.nan\n",
    "    return x\n",
    "\n",
    "# Apply parsing to all embedding columns\n",
    "for model in model_names:\n",
    "    for prefix in [\"desc\", \"reas\"]:\n",
    "        col = f\"{model}-{prefix}_emb\"\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(parse_embedding)\n",
    "\n",
    "# Compute pairwise cosine similarities for each (model1, model2) pair\n",
    "def compute_similarities(df, prefix):\n",
    "    similarities = pd.DataFrame()\n",
    "    similarities[\"video_name\"] = df[\"video_name\"]\n",
    "    for model1, model2 in itertools.combinations(model_names, 2):\n",
    "        col1 = f\"{model1}-{prefix}_emb\"\n",
    "        col2 = f\"{model2}-{prefix}_emb\"\n",
    "        sim_col = f\"{model1}_vs_{model2}_{prefix}_sim\"\n",
    "        sim_values = []\n",
    "        for v1, v2 in zip(df[col1], df[col2]):\n",
    "            try:\n",
    "                sim = cosine_similarity([v1], [v2])[0][0]\n",
    "            except:\n",
    "                sim = np.nan\n",
    "            sim_values.append(sim)\n",
    "        similarities[sim_col] = sim_values\n",
    "    return similarities\n",
    "\n",
    "# Compute similarities\n",
    "desc_sim = compute_similarities(df, \"desc\")\n",
    "reas_sim = compute_similarities(df, \"reas\")\n",
    "\n",
    "# Get anomaly columns\n",
    "anomaly_df = df[[\"video_name\"] + [f\"{model}-anomaly\" for model in model_names]]\n",
    "\n",
    "# Combine all results\n",
    "final_df = desc_sim.merge(reas_sim, on=\"video_name\").merge(anomaly_df, on=\"video_name\")\n",
    "\n",
    "# Save to Excel\n",
    "final_df.to_excel(\"video_anomaly_similarities_output.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3626d33a-992a-4e4c-8765-cdc565733184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "suffix = \"desc\"\n",
    "\n",
    "\n",
    "# Load files\n",
    "anno_df = pd.read_excel('../wound_groundtruth.xlsx')\n",
    "\n",
    "\n",
    "#To obtain video_anomaly_similarities_output_desc.xlsx, we only need to keep columns regarding description similarities in video_anomaly_similarities_output.xlsx\n",
    "\n",
    "desc_df = pd.read_excel(f'video_anomaly_similarities_output_{suffix}.xlsx')\n",
    "\n",
    "# Process video names\n",
    "anno_df['Image Name'] = anno_df['Image Name'].str.replace('.jpg', '', regex=False)\n",
    "desc_df['video_name'] = desc_df['video_name'].str.replace('.jpg', '', regex=False)\n",
    "\n",
    "# Merge\n",
    "merged_df = pd.merge(anno_df, desc_df, left_on='Image Name', right_on='video_name')\n",
    "\n",
    "# Split by type with same ratio\n",
    "train_list = []\n",
    "test_list = []\n",
    "\n",
    "for label in ['Abrasions', 'Ingrown Nails', 'Stab Wound', 'Bruises', 'Cut', 'Laceration', 'Burns']:\n",
    "    subset = merged_df[merged_df['Label'] == label]\n",
    "    train, test = train_test_split(subset, test_size=0.2, random_state=42)\n",
    "    train_list.append(train)\n",
    "    test_list.append(test)\n",
    "    print(len(test))\n",
    "\n",
    "# Combine and drop columns\n",
    "train_df = pd.concat(train_list).reset_index(drop=True).drop(columns=['Label', 'Image Name'])\n",
    "test_df = pd.concat(test_list).reset_index(drop=True).drop(columns=['Label','Image Name'])\n",
    "\n",
    "train_df.to_excel(f'train_dataset_{suffix}.xlsx', index=False)\n",
    "test_df.to_excel(f'test_dataset_{suffix}.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996eb73d-b29e-413e-8a17-1267ca4f5aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "suffix = \"desc\"\n",
    "\n",
    "# Load files\n",
    "anno_df = pd.read_excel('../wound_groundtruth.xlsx')\n",
    "desc_df = pd.read_excel('train_dataset_desc.xlsx')\n",
    "\n",
    "# Clean video names\n",
    "anno_df['Image Name'] = anno_df['Image Name'].str.replace('.jpg', '', regex=False)\n",
    "desc_df['video_name'] = desc_df['video_name'].str.replace('.jpg', '', regex=False)\n",
    "\n",
    "# Merge datasets\n",
    "merged_df = pd.merge(anno_df, desc_df, left_on='Image Name', right_on='video_name')\n",
    "\n",
    "# Optional: Create a copy of Label for stratification\n",
    "merged_df['strat_label'] = merged_df['Label']\n",
    "\n",
    "# Initialize StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Generate and save each training fold\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(merged_df, merged_df['strat_label'])):\n",
    "    train_fold = merged_df.iloc[train_idx].reset_index(drop=True)\n",
    "    # Drop unnecessary columns\n",
    "    train_fold = train_fold.drop(columns=['Label', 'Image Name', 'strat_label'])\n",
    "\n",
    "    # Save to Excel\n",
    "    train_fold.to_excel(f'train_dataset_fold{fold}_{suffix}.xlsx', index=False)\n",
    "\n",
    "print(\"✅ 5-fold training splits saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ad5e52-026a-4837-a341-e99bf14e31b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class PMF:\n",
    "    def __init__(self, num_users, num_items, num_factors, learning_rate=0.01, reg_param=0.01):\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.num_factors = num_factors\n",
    "        self.learning_rate = learning_rate\n",
    "        self.reg_param = reg_param\n",
    "        # Initialize U and V here, so they can be reset for grid search\n",
    "        self.U = np.random.normal(scale=1./self.num_factors, size=(self.num_users, self.num_factors))\n",
    "        self.V = np.random.normal(scale=1./self.num_factors, size=(self.num_items, self.num_factors))\n",
    "\n",
    "    def train(self, train_ratings, val_ratings, num_epochs=100, patience=5):\n",
    "        # train_ratings and val_ratings are lists of [i, j, rating]\n",
    "        \n",
    "        best_val_rmse = float('inf')\n",
    "        epochs_without_improvement = 0\n",
    "        \n",
    "        # Training using SGD\n",
    "        for epoch in range(num_epochs):\n",
    "            # Shuffle training data for each epoch\n",
    "            np.random.shuffle(train_ratings)\n",
    "            \n",
    "            for i, j, r_ij in train_ratings:\n",
    "                i, j = int(i), int(j)\n",
    "                prediction = self.predict(i, j)\n",
    "                error = r_ij - prediction\n",
    "\n",
    "                # Update latent factors\n",
    "                u_i = self.U[i, :].copy() # Make a copy before update\n",
    "                v_j = self.V[j, :].copy() # Make a copy before update\n",
    "                \n",
    "                self.U[i, :] += self.learning_rate * (error * v_j - self.reg_param * u_i)\n",
    "                self.V[j, :] += self.learning_rate * (error * u_i - self.reg_param * v_j)\n",
    "\n",
    "            # --- Validation Phase ---\n",
    "            val_rmse = self.compute_rmse(val_ratings)\n",
    "            print(f'Epoch: {epoch+1}, Validation RMSE: {val_rmse:.4f}')\n",
    "\n",
    "            # --- Early Stopping Logic ---\n",
    "            if val_rmse < best_val_rmse:\n",
    "                best_val_rmse = val_rmse\n",
    "                epochs_without_improvement = 0\n",
    "                # Optionally save the best model weights\n",
    "                best_U, best_V = self.U.copy(), self.V.copy()\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "            \n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f\"Stopping early after {epoch+1} epochs.\")\n",
    "                self.U, self.V = best_U, best_V # Restore best weights\n",
    "                break\n",
    "        \n",
    "        return best_val_rmse\n",
    "\n",
    "    def predict(self, i, j):\n",
    "        return np.dot(self.U[i, :], self.V[j, :])\n",
    "\n",
    "    def compute_rmse(self, ratings_data):\n",
    "        # ratings_data is a list of [i, j, rating]\n",
    "        if len(ratings_data) == 0:\n",
    "            return 0.0\n",
    "            \n",
    "        error = 0\n",
    "        for i, j, r_ij in ratings_data:\n",
    "            i, j = int(i), int(j)\n",
    "            error += (r_ij - self.predict(i, j)) ** 2\n",
    "        \n",
    "        # Return Root Mean Squared Error\n",
    "        return np.sqrt(error / len(ratings_data))\n",
    "\n",
    "    def full_matrix(self):\n",
    "        return np.dot(self.U, self.V.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff45971f-34b6-46ac-b78b-f70dad75234d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# --- Main pipeline ---\n",
    "data_dir = \"./\"  # change if needed\n",
    "os.makedirs(\"low list\", exist_ok=True)\n",
    "\n",
    "for fold in range(5):\n",
    "    suffix = f\"fold{fold}_desc\"\n",
    "    train_file = os.path.join(data_dir, f\"train_dataset_{suffix}.xlsx\")\n",
    "    test_file = os.path.join(data_dir, f\"test_dataset_desc.xlsx\")\n",
    "   \n",
    "    df = pd.read_excel(train_file, sheet_name=\"Sheet1\")\n",
    "    R = df.select_dtypes(include=[np.number]).fillna(0).values\n",
    "    xs, ys = R.nonzero()\n",
    "    ratings = np.c_[xs, ys, R[xs, ys]]\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(ratings)\n",
    "    split_index = int(0.8 * len(ratings))\n",
    "    train_ratings = ratings[:split_index]\n",
    "    val_ratings = ratings[split_index:]\n",
    "    num_users, num_items = R.shape\n",
    "\n",
    "    param_grid = {\n",
    "        'num_factors': [5, 10, 15],\n",
    "        'learning_rate': [0.005, 0.01, 0.05],\n",
    "        'reg_param': [0.01, 0.1, 0.5]\n",
    "    }\n",
    "\n",
    "    best_rmse = float('inf')\n",
    "    for f in param_grid['num_factors']:\n",
    "        for lr in param_grid['learning_rate']:\n",
    "            for reg in param_grid['reg_param']:\n",
    "                pmf = PMF(num_users, num_items, f, lr, reg)\n",
    "                rmse = pmf.train(train_ratings, val_ratings, num_epochs=100, patience=5)\n",
    "                if rmse < best_rmse:\n",
    "                    best_rmse = rmse\n",
    "                    best_model = pmf\n",
    "\n",
    "    V = best_model.V\n",
    "    V_df = pd.DataFrame(V)\n",
    "    V_df.to_excel(f\"best_V_matrix_{suffix}.xlsx\", index=False)\n",
    "\n",
    "    test_df = pd.read_excel(test_file)\n",
    "    test_numeric = test_df.select_dtypes(include=[np.number]).apply(lambda x: x.fillna(x.mean()), axis=0).values\n",
    "    video_names = test_df['video_name'].astype(str).values\n",
    "\n",
    "    mse_list = []\n",
    "    c_list = []\n",
    "    for s in test_numeric:\n",
    "        c, _, _, _ = np.linalg.lstsq(V, s, rcond=None)\n",
    "        mse = mean_squared_error(s, V @ c)\n",
    "        mse_list.append(mse)\n",
    "        c_list.append(c)\n",
    "\n",
    "    result_df = pd.DataFrame(c_list, columns=[f'c{i}' for i in range(V.shape[1])])\n",
    "    result_df['MSE'] = mse_list\n",
    "    result_df.insert(0, 'video_name', video_names)\n",
    "    result_df.to_excel(f'optimal_c_and_mse_lstsq_with_name_{suffix}.xlsx', index=False)\n",
    "\n",
    "    uncertainty_scores = np.array(mse_list)\n",
    "    for P in [5, 10, 15, 20, 25, 30, 35, 40]:\n",
    "        tau = np.percentile(uncertainty_scores, 100 - P)\n",
    "        mask = uncertainty_scores <= tau\n",
    "        kept_df = pd.DataFrame({\n",
    "            \"video_name\": video_names[mask],\n",
    "            \"uncertainty\": uncertainty_scores[mask]\n",
    "        })\n",
    "        kept_df.to_excel(f\"low list/S_low_videos_{P}_sup_{suffix}.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58b9b5b-cdde-407b-a245-651355c8937b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "for fold in range(5):\n",
    "    suffix = f\"fold{fold}_desc\"\n",
    "    \n",
    "    # Load files\n",
    "    V_df = pd.read_excel(f'best_V_matrix_{suffix}.xlsx')\n",
    "    test_df = pd.read_excel(f'train_dataset_{suffix}.xlsx')\n",
    "  \n",
    "    \n",
    "    # Prepare matrix V and numeric part of test dataset\n",
    "    V = V_df.values\n",
    "    test_numeric_df = test_df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Fill missing values with column mean\n",
    "    test_numeric_df = test_numeric_df.apply(lambda x: x.fillna(x.mean()), axis=0)\n",
    "    S_numeric = test_numeric_df.values\n",
    "    \n",
    "    # Extract video names (assumes column 'video_name' exists)\n",
    "    video_names = test_df['video_name'].values\n",
    "    \n",
    "    # Solve for c minimizing ||s - Vc||^2 using least squares\n",
    "    mse_list = []\n",
    "    c_list = []\n",
    "    \n",
    "    for s in S_numeric:\n",
    "        c, residuals, _, _ = np.linalg.lstsq(V, s, rcond=None)\n",
    "        s_pred = V @ c\n",
    "        mse = mean_squared_error(s, s_pred)\n",
    "        mse_list.append(mse)\n",
    "        c_list.append(c)\n",
    "    \n",
    "    # Output results with video names\n",
    "    result_df = pd.DataFrame(c_list, columns=[f'c{i}' for i in range(V.shape[1])])\n",
    "    result_df['MSE'] = mse_list\n",
    "    result_df.insert(0, 'video_name', video_names)\n",
    "    \n",
    "    result_df.to_excel(f'optimal_c_and_mse_lstsq_with_name_{suffix}_train.xlsx', index=False)\n",
    "\n",
    "\n",
    "    uncertainty_scores = np.array(mse_list)\n",
    "    for P in [5, 10, 15, 20, 25, 30, 35, 40]:\n",
    "        tau = np.percentile(uncertainty_scores, 100 - P)\n",
    "        mask = uncertainty_scores <= tau\n",
    "        kept_df = pd.DataFrame({\n",
    "            \"video_name\": video_names[mask],\n",
    "            \"uncertainty\": uncertainty_scores[mask]\n",
    "        })\n",
    "        kept_df.to_excel(f\"low list/S_low_videos_{P}_sup_{suffix}_train.xlsx\", index=False)\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755f4156-0600-4a9c-a9d4-832699fea6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load files\n",
    "groundtruth_path = \"wound_groundtruth.xlsx\"\n",
    "predictions_path = \"../../LLM_output_generation/extracted_video_anomalies_all_models.xlsx\"\n",
    "\n",
    "groundtruth_df = pd.read_excel(groundtruth_path)\n",
    "predictions_df = pd.read_excel(predictions_path, sheet_name=\"Sheet1\")\n",
    "\n",
    "# Normalize filenames and labels\n",
    "groundtruth_df[\"filename\"] = groundtruth_df[\"Image Name\"].str.strip().str.replace('.jpg', '', regex=False)\n",
    "groundtruth_df[\"true_label\"] = groundtruth_df[\"Label\"].str.strip()\n",
    "groundtruth_df[\"filename\"] = groundtruth_df[\"filename\"].astype(str)\n",
    "predictions_df[\"filename\"] = predictions_df[\"video_name\"].str.strip().str.replace('.jpg', '', regex=False)\n",
    "\n",
    "# Mapping model names to column suffixes\n",
    "model_suffixes = {\n",
    "    \"claude3.5sonnet\": \"claude3.5sonnet-anomaly\",\n",
    "    \"claude3.5\": \"claude3.5-anomaly\",\n",
    "    \"claude3.7\": \"claude3.7-anomaly\",\n",
    "    \"gpt4o-mini\": \"gpt4o-mini-anomaly\",\n",
    "    \"gpt4o\": \"gpt4o-anomaly\"\n",
    "}\n",
    "\n",
    "# Output directory\n",
    "output_dir = \"../model_predictions\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Process and save CSVs\n",
    "for model_name, col_name in model_suffixes.items():\n",
    "    model_df = predictions_df[[\"filename\", col_name]].rename(columns={\n",
    "        \"filename\": \"Video Name\",\n",
    "        col_name: \"Predicted Label\"\n",
    "    })\n",
    "    groundtruth_subset = groundtruth_df[[\"filename\", \"true_label\"]].rename(columns={\n",
    "        \"filename\": \"Video Name\",\n",
    "        \"true_label\": \"True Label\"\n",
    "    })\n",
    "    merged = pd.merge(model_df, groundtruth_subset, on=\"Video Name\", how=\"left\")\n",
    "    merged.to_csv(os.path.join(output_dir, f\"vad_results_{model_name}.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4b6991-33ba-4a70-8575-1b052f831c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Model names\n",
    "model_names = [\n",
    "    \"claude3.5sonnet\",\n",
    "    \"claude3.5\",\n",
    "    \"claude3.7\",\n",
    "    \"gpt4o-mini\",\n",
    "    \"gpt4o\"\n",
    "]\n",
    "# P values representing percent of videos excluded as high-uncertainty\n",
    "P_set = [5, 10, 15, 20, 25, 30, 35, 40]\n",
    "\n",
    "\n",
    "\n",
    "for fold in range(5):\n",
    "    suffix = f\"fold{fold}_desc\"\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # Store results for each P\n",
    "    all_results = []\n",
    "    \n",
    "    for P in P_set:\n",
    "        \n",
    "        try:\n",
    "            low = pd.read_excel(f'low list/S_low_videos_{P}_sup_{suffix}.xlsx')\n",
    "        except Exception as e:\n",
    "            print(f\"[Error] Failed to load S_low_videos_{P}_trace.xlsx: {e}\")\n",
    "            continue\n",
    "        \n",
    "        low_list = low['video_name'].str.replace('.mp4', '', regex=False)\n",
    "        \n",
    "        # Store predictions per model\n",
    "        model_preds = {}\n",
    "        \n",
    "        for model in model_names:\n",
    "            try:\n",
    "                df = pd.read_csv(f'../model_predictions/vad_results_{model}.csv')\n",
    "                df['Video Name'] = df['Video Name'].str.replace('.jpg', '', regex=False)\n",
    "                df = df[df['Video Name'].isin(low_list)]\n",
    "                model_preds[model] = df[['Video Name', 'Predicted Label']].set_index('Video Name')\n",
    "            except Exception as e:\n",
    "                print(f\"[Error] Failed to process {model}: {e}\")\n",
    "        \n",
    "        # Merge all predictions\n",
    "        merged = pd.DataFrame(index=low_list)\n",
    "        for model in model_names:\n",
    "            merged[model] = model_preds.get(model, pd.DataFrame()).reindex(low_list)['Predicted Label']\n",
    "        \n",
    "        # Drop rows with missing predictions\n",
    "        merged = merged.dropna()\n",
    "        \n",
    "        if merged.empty:\n",
    "            print(f\"[Warning] No valid data for P={P}, skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Load ground truth from claude-3-5-sonnet\n",
    "        gt_df = pd.read_csv('../model_predictions/vad_results_claude3.5sonnet.csv')\n",
    "        gt_df['Video Name'] = gt_df['Video Name'].str.replace('.mp4', '', regex=False)\n",
    "        gt_df = gt_df[gt_df['Video Name'].isin(merged.index)]\n",
    "        ground_truth = gt_df.set_index('Video Name')['True Label']\n",
    "        \n",
    "        # Majority voting\n",
    "        majority_vote = merged.mode(axis=1)[0]\n",
    "        \n",
    "        # Align ground truth\n",
    "        y_true = ground_truth.loc[merged.index]\n",
    "        \n",
    "        # Compute metrics\n",
    "        acc = accuracy_score(y_true, majority_vote)\n",
    "        prec = precision_score(y_true, majority_vote,  average='macro',zero_division=0)\n",
    "        rec = recall_score(y_true, majority_vote, average='macro', zero_division=0)\n",
    "        f1 = f1_score(y_true, majority_vote, average='macro', zero_division=0)\n",
    "        \n",
    "        all_results.append({\n",
    "            'P': P,\n",
    "            'Num Videos': len(merged),\n",
    "            'Accuracy': acc,\n",
    "            'Precision': prec,\n",
    "            'Recall': rec,\n",
    "            'F1-Score': f1\n",
    "        })\n",
    "        \n",
    "        print(f\"[P={P}] Processed {len(merged)} videos | Accuracy: {acc:.4f}, F1-Score: {f1:.4f}\")\n",
    "    \n",
    "    # Save final results to one Excel file\n",
    "    result_df = pd.DataFrame(all_results)\n",
    "    result_df.to_excel(f'low list/VAD_Majority_Voting_Summary_sup_{suffix}.xlsx', index=False)\n",
    "    print(\"[Saved] VAD_Majority_Voting_Summary.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0004804-2ce1-48f4-9217-f975ebfb6d4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Model names\n",
    "model_names = [\n",
    "    \"claude3.5sonnet\",\n",
    "    \"claude3.5\",\n",
    "    \"claude3.7\",\n",
    "    \"gpt4o-mini\",\n",
    "    \"gpt4o\"\n",
    "]\n",
    "# P values representing percent of videos excluded as high-uncertainty\n",
    "P_set = [5, 10, 15, 20, 25, 30, 35, 40]\n",
    "\n",
    "\n",
    "\n",
    "for fold in range(5):\n",
    "    suffix = f\"fold{fold}_desc\"\n",
    "\n",
    "\n",
    "    # Store results for each P\n",
    "    all_results = []\n",
    "    \n",
    "    for P in P_set:\n",
    "        \n",
    "        try:\n",
    "            low = pd.read_excel(f'low list/S_low_videos_{P}_sup_{suffix}_train.xlsx')\n",
    "        except Exception as e:\n",
    "            print(f\"[Error] Failed to load S_low_videos_{P}_trace.xlsx: {e}\")\n",
    "            continue\n",
    "        \n",
    "        low_list = low['video_name'].str.replace('.mp4', '', regex=False)\n",
    "        \n",
    "        # Store predictions per model\n",
    "        model_preds = {}\n",
    "        \n",
    "        for model in model_names:\n",
    "            try:\n",
    "                df = pd.read_csv(f'../model_predictions/vad_results_{model}.csv')\n",
    "                df['Video Name'] = df['Video Name'].str.replace('.jpg', '', regex=False)\n",
    "                df = df[df['Video Name'].isin(low_list)]\n",
    "                model_preds[model] = df[['Video Name', 'Predicted Label']].set_index('Video Name')\n",
    "            except Exception as e:\n",
    "                print(f\"[Error] Failed to process {model}: {e}\")\n",
    "        \n",
    "        # Merge all predictions\n",
    "        merged = pd.DataFrame(index=low_list)\n",
    "        for model in model_names:\n",
    "            merged[model] = model_preds.get(model, pd.DataFrame()).reindex(low_list)['Predicted Label']\n",
    "        \n",
    "        # Drop rows with missing predictions\n",
    "        merged = merged.dropna()\n",
    "        \n",
    "        if merged.empty:\n",
    "            print(f\"[Warning] No valid data for P={P}, skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Load ground truth from claude-3-5-sonnet\n",
    "        gt_df = pd.read_csv('../model_predictions/vad_results_claude3.5sonnet.csv')\n",
    "        gt_df['Video Name'] = gt_df['Video Name'].str.replace('.mp4', '', regex=False)\n",
    "        gt_df = gt_df[gt_df['Video Name'].isin(merged.index)]\n",
    "        ground_truth = gt_df.set_index('Video Name')['True Label']\n",
    "        \n",
    "        # Majority voting\n",
    "        majority_vote = merged.mode(axis=1)[0]\n",
    "        \n",
    "        # Align ground truth\n",
    "        y_true = ground_truth.loc[merged.index]\n",
    "        \n",
    "        # Compute metrics\n",
    "        acc = accuracy_score(y_true, majority_vote)\n",
    "        prec = precision_score(y_true, majority_vote,  average='macro',zero_division=0)\n",
    "        rec = recall_score(y_true, majority_vote, average='macro', zero_division=0)\n",
    "        f1 = f1_score(y_true, majority_vote, average='macro', zero_division=0)\n",
    "        \n",
    "        all_results.append({\n",
    "            'P': P,\n",
    "            'Num Videos': len(merged),\n",
    "            'Accuracy': acc,\n",
    "            'Precision': prec,\n",
    "            'Recall': rec,\n",
    "            'F1-Score': f1\n",
    "        })\n",
    "        \n",
    "        print(f\"[P={P}] Processed {len(merged)} videos | Accuracy: {acc:.4f}, F1-Score: {f1:.4f}\")\n",
    "    \n",
    "    # Save final results to one Excel file\n",
    "    result_df = pd.DataFrame(all_results)\n",
    "    result_df.to_excel(f'low list/VAD_Majority_Voting_Summary_sup_{suffix}_train.xlsx', index=False)\n",
    "    print(\"[Saved] VAD_Majority_Voting_Summary.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e45b037-10f6-49d8-9b0e-647dc08356dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#suffix = \"desc\"\n",
    "\n",
    "\n",
    "for fold in range(5):\n",
    "    suffix = f\"fold{fold}_desc\"\n",
    "\n",
    "\n",
    "    \n",
    "    # Load the summary files\n",
    "    majority_df = pd.read_excel(f'low list/VAD_Majority_Voting_Summary_sup_{suffix}.xlsx')\n",
    "    \n",
    "    # Extract metrics\n",
    "    P_values = majority_df['P']\n",
    "    overall_accuracy = majority_df['Accuracy']\n",
    "    # recall = majority_df['Recall']\n",
    "    # vague_abnormal_accuracy = vague_df['Accuracy']\n",
    "    \n",
    "    # Combine into final DataFrame\n",
    "    out_df = pd.DataFrame({\n",
    "        'P': P_values,\n",
    "        'Overall Accuracy': overall_accuracy\n",
    "        # 'Recall': recall,\n",
    "        # 'Vague Abnormal Accuracy': vague_abnormal_accuracy\n",
    "    })\n",
    "    \n",
    "    # Remove final two rows\n",
    "    out_df = out_df.iloc[:, :]\n",
    "    \n",
    "    # Save to xlsx\n",
    "    out_df.to_excel(f'low list/results_P_sup_{suffix}.xlsx', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
